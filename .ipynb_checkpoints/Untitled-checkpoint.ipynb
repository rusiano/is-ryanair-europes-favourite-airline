{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('memoria.txt', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'EIT Data Science\\r\\nUniversidad Polit\\xe9cnica de Madrid\\r\\nEscuela T\\xe9cnica Superior de\\r\\nIngenieros Inform\\xe1ticos\\r\\nTRABAJO FIN DE MASTER\\r\\nLearning Analytics for Data-Driven Decision Support\\r\\nSystem in SPOCs: A Case Study\\r\\nAutor: Massimiliano Russo\\r\\nSupervisor: Prof. Ernestina Menasalvas\\r\\nMADRID, SEPTEMBER 9, 2018\\r\\n\\r\\nIn memory of my beloved grandad\\r\\nGennaro Russo,\\r\\nwhose iconic encyclopedic little notebook\\r\\nis the first dataset ever\\r\\nI put my hands on.\\r\\n\\r\\nAcknowledgements\\r\\nFirst of all, I would like to give my special thanks to Dr. Martin Rodriguez Jugo from\\r\\nIEXL for having accepted me on-board and for having given me plenty of confidence\\r\\nright from the start. Also, I would like to express my sincere gratitude to my direct supervisor\\r\\nat IE Dr. Francisco Mach\\xedn Aragon\\xe9s for the faith and trust he placed in me in\\r\\nall these months. I extend my thanks to Juan Luis Rivero Antonio for all the help and the\\r\\nadvices he gave me.\\r\\nMy sincere thanks also go to Prof. Massimiliano Zanin for having always been a solid\\r\\npoint of reference for me even before the presence of a Thesis supervisor and for being\\r\\na constant source of inspiration thanks to his insightful comments. Together with him, I\\r\\ninfinitely thank Prof. Ernestina Menasalvas for having accepted to supervise my Thesis\\r\\nwith such short notice. Honestly, I think I could not have made it without her knowledge,\\r\\npatience and time.\\r\\nAnother special mention goes to my girlfriend Iza, for the unconditional love, care\\r\\nand motivation she always gave me and for having lived with me through the stress of the\\r\\ncompletion of this work.\\r\\nFinally, I cannot conclude without saying that I am much obliged to all my family for\\r\\nthe silent yet heart-felt support.\\r\\n\\r\\nAbstract\\r\\nAqu\\xed el texto del abstract.\\r\\nPalabras clave: palabra 1, palabra 2, palabra 3. . .\\r\\nAbstract\\r\\nThanks to the growing diffusion of Learning Management Systems (LMSs), more and\\r\\nmore data describing students\\x92 online behaviour is becoming available every day. This\\r\\ninformation can be extremely useful for educational institutions to monitor and get insight\\r\\nabout their programmes and eventually take preventive or corrective actions. We\\r\\ntherefore present the full development of a data-mining project carried out for IE Exponential\\r\\nLearning (abbr. IEXL, a business unit within renowned IE Business School) with\\r\\nthe main objective of offering them a data-driven decision support system. We used data\\r\\nfrom the last six intakes of a small private online course (SPOC) they offer. As major contributions\\r\\nof this work, we firstly propose a new definition of students\\x92 knowledge and engagement\\r\\nas distinct sets of variables. Together with this, we also present a methodology\\r\\nto extract such features importances directly from data, which allowed us to eventually\\r\\nprove the correlation among these two concepts. Secondly, we discuss the realization of a\\r\\npredictive modeling of students\\x92 grades and its potential use as an early-warning system.\\r\\nTo conclude, we show how the little amount of data affects negatively the quality and the\\r\\napplicability of the results, which thus calls for future further improvement.\\r\\nKeywords: learning analytics, learning management systems, decision support system,\\r\\nstudent performance, higher education\\r\\n\\r\\nCONTENTS\\r\\n1 Introduction 1\\r\\n1.1 Rationale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\r\\n1.2 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\r\\n1.2.1 Business Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\r\\n1.2.2 Data Mining Goals . . . . . . . . . . . . . . . . . . . . . . . . . 4\\r\\n2 State of the Art 7\\r\\n2.1 Knowledge & Engagement . . . . . . . . . . . . . . . . . . . . . . . . . 7\\r\\n2.2 Predictive Modelling for Student Success . . . . . . . . . . . . . . . . . 10\\r\\n3 Methodology 13\\r\\n3.1 CRISP-DM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\r\\n3.2 Data Mining Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\r\\n3.2.1 Data Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\r\\n3.2.2 Dimensionality Reduction . . . . . . . . . . . . . . . . . . . . . 16\\r\\n3.2.3 Resampling & Synthetic Data Generation . . . . . . . . . . . . . 18\\r\\n3.2.4 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\r\\n3.2.5 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\r\\n3.2.6 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\r\\n4 Development 25\\r\\n4.1 Data Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\r\\n4.1.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\r\\n4.1.2 Data Description . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\r\\n4.1.3 Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . 44\\r\\n4.2 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\r\\n4.2.1 Features Extraction . . . . . . . . . . . . . . . . . . . . . . . . . 50\\r\\n4.2.2 Modeling Datasets Generation . . . . . . . . . . . . . . . . . . . 63\\r\\n4.2.3 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 64\\r\\n4.3 Modeling & Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\r\\n4.3.1 Independent t-Test for Differently Performing Groups of Students 68\\r\\n4.3.2 K&E Features Importance . . . . . . . . . . . . . . . . . . . . . 70\\r\\n4.3.3 Students\\x92 Performance Prediction . . . . . . . . . . . . . . . . . 75\\r\\n4.4 Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\r\\niii\\r\\n5 Conclusions & Future Work 81\\r\\n5.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\r\\n5.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\r\\niv\\r\\nLIST OF FIGURES\\r\\n3.1 CRISP-DM Flowchart. . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\r\\n3.2 Curse of Dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\r\\n4.1 UML Diagram for Automated Data Collection and Storage Software Tool 28\\r\\n4.2 Items: Count Plot by Type . . . . . . . . . . . . . . . . . . . . . . . . . 33\\r\\n4.3 Authentication Events: Count Plot by Event Type (Login/Logout) . . . . 37\\r\\n4.4 Page Views: Preliminary Data Exploration and Filtering Steps . . . . . . 39\\r\\n4.5 Submissions. Assignments Grades Comparison for Consecutive Intakes . 42\\r\\n4.6 Item Views: Insightful Visualizations . . . . . . . . . . . . . . . . . . . . 48\\r\\n4.7 Distribution of Writing Skills Features . . . . . . . . . . . . . . . . . . . 56\\r\\n4.8 Distribution of Time-On-Task Features . . . . . . . . . . . . . . . . . . . 61\\r\\n4.9 Comparison of Discretization Strategies for Students\\x92 Grades . . . . . . . 62\\r\\n4.10 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\r\\n4.11 Features Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\r\\n4.12 Data Scaling Techniques Comparison . . . . . . . . . . . . . . . . . . . 69\\r\\n4.13 Knowledge & Engagement: Preliminary Clustering Analysis . . . . . . . 72\\r\\n4.14 Knowledge & Engagement: Ultimate Clusters Overview . . . . . . . . . 73\\r\\n4.15 Knowledge & Engagement: Extracted Features Importances . . . . . . . 75\\r\\n4.16 Knowledge & Engagement: Correlation and Regression Analysis . . . . . 76\\r\\n4.17 Performance Prediction: Best Models F2 Scores Comparison . . . . . . . 78\\r\\n4.18 Performance Prediction: best models accuracy comparison . . . . . . . . 79\\r\\n4.19 Performance Prediction: Best Models Features Importances . . . . . . . . 79\\r\\nv\\r\\n\\r\\nLIST OF TABLES\\r\\n2.1 Overview of Predictive Modeling for Student Success in Literature . . . . 11\\r\\n2.2 Variables Used in Literature for Student Success Prediction . . . . . . . . 12\\r\\n4.1 Raw Datasets Available Through Canvas API . . . . . . . . . . . . . . . 26\\r\\n4.2 Assignments: Fields Description . . . . . . . . . . . . . . . . . . . . . . 30\\r\\n4.3 Assignments: Issues with Pubblication Dates . . . . . . . . . . . . . . . 31\\r\\n4.4 Assignments: Assignments Names Standardization . . . . . . . . . . . . 32\\r\\n4.5 Assignments: Issues with Position and Group Position . . . . . . . . . . 32\\r\\n4.6 Assignments: Issues with Groups Structures . . . . . . . . . . . . . . . . 33\\r\\n4.7 Items: Modules Names Comparison Across Different Intakes . . . . . . . 34\\r\\n4.8 Items: Publication Dates . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\r\\n4.9 Items: Fields Description . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\r\\n4.10 Quiz Questions: Fields Description . . . . . . . . . . . . . . . . . . . . . 37\\r\\n4.11 Page views: Fields Description . . . . . . . . . . . . . . . . . . . . . . . 38\\r\\n4.12 Discussion Topics & Main Posts: Example Datasets . . . . . . . . . . . . 40\\r\\n4.13 Discussion Messages: Fields Description . . . . . . . . . . . . . . . . . 41\\r\\n4.14 Submissions: Fields Description . . . . . . . . . . . . . . . . . . . . . . 43\\r\\n4.15 Students\\x92 Grades: Issues with Average Scores and Points Possible . . . . 43\\r\\n4.16 Peer Reviews: Example Dataset . . . . . . . . . . . . . . . . . . . . . . 44\\r\\n4.17 Peer Reviews Comments: Fields Description . . . . . . . . . . . . . . . 45\\r\\n4.18 Items: URL-Related Fields Comparison . . . . . . . . . . . . . . . . . . 46\\r\\n4.19 Ratings & Unread Posts: Example Datasets . . . . . . . . . . . . . . . . 47\\r\\n4.20 Extracted Features Description . . . . . . . . . . . . . . . . . . . . . . . 52\\r\\n4.21 Words Importance: Example List . . . . . . . . . . . . . . . . . . . . . . 55\\r\\n4.22 Grading Schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\r\\n4.23 Extracted Features Variance . . . . . . . . . . . . . . . . . . . . . . . . 67\\r\\n4.24 Independent t-Test Results: Passing vs Failing Students) . . . . . . . . . 70\\r\\n4.25 Features Repartition Between Knowledge and Engagement . . . . . . . . 71\\r\\n4.26 Cross-validation results: Knowledge & Engagement . . . . . . . . . . . . 74\\r\\n4.27 Cross-validation results: Performance Prediction . . . . . . . . . . . . . 80\\r\\nvii\\r\\n1 INTRODUCTION\\r\\nThe work here presented is the result of a six months internship at IE Exponential Learning,\\r\\na quite small business unit within Instituto de Empresa in Madrid. Instituto de Empresa\\r\\n(from here on abbreviated with the initials IE) is a world\\x92s leader institution in the\\r\\nfield of higher education and they offer a wide selection of programs on different topics,\\r\\nwith different durations and formats. Within IE, IE Exponential Learning (abbreviated\\r\\nIEXL) is specifically oriented towards intensive courses and bootcamps targeted mostly\\r\\nto professionals and workers. Among all the products they offer, the work of these months\\r\\nfocused specifically on courses called HIOPs (High Impact On-Line Programs) which are\\r\\nintensive 5-weeks fully-on-line courses.\\r\\nIn this chapter, a first overlook of the background of IE and IEXL together with a\\r\\ndiscussion about the reasons for the development of such a data mining project is given\\r\\n(Section 1.1), followed by the explanation of its objectives, both from a business point of\\r\\nview as well as from an academic one (Section 1.2).\\r\\n1.1 Rationale\\r\\nIE Exponential Learning is a quite small unit that offers alternative programs in the form\\r\\nof intensive fully-on-line courses called HIOPs (High Impact On-Line Programs). Although\\r\\namong fully on-line courses the most known format is the MOOC (Massive Open\\r\\nOnline Course), that is freely available courses designed for thousands, hundreds of thousands\\r\\nor even million of students (in most of cases) without any time constraint, HIOPs\\r\\ncurrently represent the exact opposite: they are private, they are attended by a small\\r\\namount of students and they are much more intensive as they are only 5 weeks long.\\r\\nBeing HIOPs fully on-line courses, it means that in principle all students\\x92 activity\\r\\nand interactions with course materials, staff and peers is performed on-line and thus is\\r\\ntraceable and that data can be collected and analyzed to extract useful insight. Also, given\\r\\nthat IEXL is a relatively new and dynamic unit, it is still under development and it is more\\r\\nprone to test new approaches and use the power of data to improve their products and\\r\\nservices. Because of this, HIOPs represent a sort of sandbox and have thus been chosen\\r\\nas a starting point for the acquisition, testing and deployment of data science mindset,\\r\\ntechniques and tools.\\r\\nThis context represents the very first raison d\\x92\\xeatre of this project: while most of the\\r\\nresearch went either in the direction of blended undergrad programs or massive on-line\\r\\nopen courses, no valuable insights are available for this restricted type of courses. Thus,\\r\\n1\\r\\nthe need to develop a very ad-hoc analysis specifically for HIOPs.\\r\\nThe fact that HIOPs are entirely on-line courses means also that all students interactions\\r\\nare (possibly) stored somewhere and can be accessed. At the time of writing, the\\r\\nLearning Management System (abbr. LMS - that is the web platform where courses are\\r\\nmade available) chosen by IE for HIOPs is Canvas. This means that to get all course and\\r\\nstudents data it is necessary to retrieve it from Canvas servers.\\r\\nFor, as always, the main obstacle to data analysis is getting, cleaning and structuring\\r\\nthe data, data architecture, their collection (and automation) is the another fundamental\\r\\nissue. Indeed, before the development and implementation of the project here described\\r\\nthe only reports or pieces of insights available were coming from small batches of data\\r\\nmanually downloaded at irregular intervals of time. Being it clearly distant from an optimal\\r\\napproach, it was a very long and unpleasant task that was requiring more time than\\r\\nsupposed to the person in charge, whose realization was dependent on the said person\\x92s\\r\\nload of work. On top of that, up to that time the type of data analyzed was either the one\\r\\nalready provided by the LMS in the form of downloadable reports or scraped using external\\r\\ntools for the browser. It was then evident that the automation of the previous process\\r\\nand the expansion of the set of retrieved information could go along together. Expanding\\r\\nand automatizing this process made necessary a fully understanding of Canvas LMS\\r\\nREST API to be aware of which data are (and - most importantly - are not!) available, as\\r\\nwell as some deeper thinking about how data can be structured and stored.\\r\\nOther important aspects to take into account are the purpose and the beneficiaries of\\r\\nthe analysis here above mentioned. To better understand this point, a small explanation\\r\\nof the parts involved, how work and responsibilities are distributed, as well as some clarification\\r\\nabout some popular words is required. The first terms to make clear are course\\r\\nand intake: despite the general understanding, course could be interpreted as \"topic\",\\r\\nlike for example \"Digital Marketing\" or \"Leading Innovation\". A course can then have\\r\\nmultiple intakes, that is \"sessions\". The intakes are generally indicated by the corresponding\\r\\ncourse name and the month and year when they start (e.g. Digital Marketing\\r\\n09.2017). Depending on its popularity and demand, the same course can have 3 or 4 nonoverlapping\\r\\nintakes per year. However, different course may have overlapping intakes,\\r\\nmeaning that at the same time there are (often) more intakes open. While professors and\\r\\nassistant professors are course-specific and follow one intake at a time, there is another\\r\\nprofessional figure, namely the project manager that is responsible to inform, help and\\r\\nmonitor all the students from multiple courses and intakes, as well as to take care of more\\r\\ntechnical matters such as deadline extensions. Indeed, while the main professor only provides\\r\\nthe video-lectures, the assistant professors are responsible to solve students\\x92 doubts\\r\\nand questions regarding exclusively the content of the lectures; project managers are thus\\r\\naccountable for all the other issues.\\r\\nAlthough single intakes are currently attended by a small amount of people, for a\\r\\nproject manager this number (and consequently the workload) easily grows with more\\r\\nintakes open and this represents an important bottleneck for potential future growth. If it\\r\\nis true that a project manager can effectively handle n students at a time, what happens\\r\\nif in one or two years this number grows by 50% or 100% or the institution aims at such\\r\\na growth? Since incorporating new project managers linearly to the number of students\\r\\n2\\r\\nis not scalable, this situation implies the need for a tool for the monitoring of students\\x92\\r\\nactivity that could enable the identification of struggling or at-risk students. It is important\\r\\nto note that besides the advantages for the staff, this tool brings in value for the students\\r\\nand the institution as a whole, too. Indeed, in an on-line environment where there is less\\r\\nlittle interaction and little opportunity for staff and professors to understand who is really\\r\\nstruggling, as well as for students to evaluate and compare their performance with those\\r\\nof their peers, the proposed solution can give students that feeling of safety, that certainty\\r\\nof not being left behind. This is not a negligible detail especially in the case of private and\\r\\nexpensive courses, whose purchase may represent an important investment. Hence, apart\\r\\nfrom its pedagogical utility, the tool is significant from a pure marketing aspect, too, as it\\r\\nadds value to the service offered.\\r\\n1.2 Objectives\\r\\nComplying with the methodology proposed by CRISP-DM[48] (see more in Section 3.1),\\r\\nthe most well-known open standard process model for data mining projects, the first step\\r\\nin a project is to understand its objectives from a business point of view (i.e. from client\\x92s\\r\\nperspective) and only later, where possible, to translate them in specific data mining goals.\\r\\nFollowing this structure, the next sections will first examine in depth the business goals,\\r\\nfollowed by data mining goals.\\r\\n1.2.1 Business Goals\\r\\nOn the business side, the main goal is to have a decision support system that could help\\r\\nmanagers, teachers and staff in general to take decisions using the power of the data\\r\\navailable. Yet it is possible to divide it into these following objectives:\\r\\nBG.1. Improvement of previous models for students\\x92 learning journey monitoring\\r\\nThe only model deployed before the implementation of the work here presented\\r\\nwas an algorithm that scored engagement and knowledge for each student and\\r\\nplotted these two values in a 2D space, with the goal of helping academic staff\\r\\nmonitoring students\\x92 learning journey and identifying students at risk. However,\\r\\nthe methodologies used to collect data, as well as the variables and the algorithm\\r\\nwere still very naive, for example assigning arbitrary weights to variables. The\\r\\ngoal here is then to improve this model, investigating if there are more realistic\\r\\nand accurate ways to define and score students\\x92 knowledge and engagement. The\\r\\nkey performance indicators should address four main macro-areas:\\r\\n(a) assignments & quizzes: although knowledge is not only about grades, it is\\r\\nstill a good part undoubtedly;\\r\\n(b) discussion forums & communication: in an on-line environment discussion\\r\\nforums become arguably the main place for students to interact and\\r\\nexpress themselves. The participation and the quality of the interventions\\r\\nare considered to be good measures of students\\x92 engagement and knowledge,\\r\\nrespectively;\\r\\n3\\r\\n(c) contents & resources: as in every on-line course, video contents and other\\r\\nresources are the means through which students learn. Because of this their\\r\\nconsumption must be measured;\\r\\n(d) time on platform: time can be used as a measure of activity, thus it must be\\r\\ntaken into account.\\r\\nBG.2. Early identification of at-risk or low-performing students\\r\\nMonitoring knowledge and engagement as just explained may not be enough. Staff\\r\\njudgment may fail. Hence, IEXL would like to have an auxiliary tool that can give\\r\\nsuggestions about which students are falling behind and that staff could use to\\r\\ntake decisions, especially at very early stages of the courses. Since intakes are\\r\\nonly five weeks long, predictions should be generated before the beginning of the\\r\\nfourth week at last, since later there would not be any room for changes; the best\\r\\nscenario would be having predictions within the third week of course.\\r\\nKey element for success here is not over-estimating student performance, being\\r\\ncautious: it is better to offer a bit more of assistance to a not-at-risk student than\\r\\nneglecting a needy one. Also, it must be noted that given the will by IEXL to push\\r\\nits students to the best possible results, the goal of prediction is not only to identify\\r\\nfailing students, but rather all students that are performing worse than they should.\\r\\nHere-hence, the importance of the two categories: at-risk and low-performing.\\r\\nBG.3. Identification of key behavioural differences between differently performing\\r\\nstudents\\r\\nTeachers and staff would be also interested in finding out which factors draw an\\r\\nimportant line between students that perform differently, apart from grades. For\\r\\nexample, in which areas do passing and failing students differ more? Which are\\r\\nthe factors that make a difference between excellent students and good ones or,\\r\\nsimilarly, between failing students and low-performing ones?\\r\\nBG.4. Improvement of courses design\\r\\nManagers and course designers are interested in getting insights about HIOPs to\\r\\nfind out how students behave and how courses design could be changed and improved\\r\\nto adapt to students needs.\\r\\n1.2.2 Data Mining Goals\\r\\nThe business goals just mentioned rise can be translated into a set of data mining goals,\\r\\neach implying an action plan and a set of criteria for evaluation:\\r\\nDG.1. Extraction of engagement & knowledge features\\r\\nBusiness Goals Tackled: BG.1.\\r\\nPreviously, engagement took into account only the total number of page visited\\r\\nand the number of interactions with the platform (as in assignments submissions\\r\\nor messages postings), while knowledge was computed simply as the weighted\\r\\naverage of all students\\x92 grades. The idea is too go beyond these simple representations\\r\\nby adding broader and more complex variables (Section 4.2.1), using new\\r\\n4\\r\\nand more data (Section 4.2) than in the past. The extracted features should measure\\r\\nthe KPIs identified by the staff: assignments and quizzes, communication,\\r\\nlearning resources and time on platform.\\r\\nDG.2. Extraction of engagement & knowledge features importance\\r\\nBusiness Goals Tackled: BG.1., BG.2.\\r\\nAlthough engagement and knowledge can be interpreted as two sets of variables, it\\r\\nshould also be possible to score them for one student at any given point in time. To\\r\\ndo this properly, each variable considered should have a weight proportional to its\\r\\nimportance. While so far these values have been assigned arbitrarily (all variables\\r\\nhave same weight), the challenge is to extract them from data. To do this, students\\r\\nshould be first assigned to different groups based on their levels of knowledge\\r\\nand engagement using either clustering or arbitrary labels; then, a classification\\r\\nalgorithm should be trained to classify students maximizing accuracy; finally, the\\r\\ncoefficients or the importances of the features should be extracted and used as\\r\\nimportance weights.\\r\\nDG.3. Proof of correlation between knowledge & engagement\\r\\nBusiness Goals Tackled: BG.1., BG.4.\\r\\nEven though the correlation between knowledge and engagement is often considered\\r\\nobvious, some scholars [15][16][37] did not find such evidence, so it must\\r\\nbe proven. This is particularly true in the context of on-line courses attended by\\r\\nworking people, who may learn and study a lot although not being very engaged\\r\\nwith the on-line platform. Once all the weights for engagement and knowledge\\r\\nfeatures are set, it is possible to compute an engagement and knowledge score per\\r\\neach student as a weighted sum of his features. The two variable can be finally\\r\\ncompared to investigate if they are correlated and how.\\r\\nDG.4. Independent t-Test for groups of students based on final score\\r\\nBusiness Goals Tackled: BG.3.\\r\\nIndependent t-Test is a statistical test to validate whether the mean values of a\\r\\nvariable across two distinct groups actually differ. In case it does, it is possible to\\r\\ninfer that the given variable is a key factor in discern among the two groups. By\\r\\ndefining the proper groups it is possible to answer the questions defined in BG.3..\\r\\nThe general condition to reject the null hypotesis (that is, the two groups have\\r\\nequal mean) is the p-value being lower than 0.025.\\r\\nDG.5. Training of a model for early (and \"safe\") predictions of final student grades\\r\\nBusiness Goals Tackled: BG.2.\\r\\nUsing the engagement and knowledge features extracted all together, it is possible\\r\\nto train a model to predict students\\x92 final grades. Since it is important to make\\r\\nearly predictions, the data used for training must contain information about students\\x92\\r\\nbehaviour and activity over limited amount of days and not over all the\\r\\nduration of the courses. This introduces the need for the features to be actually\\r\\n5\\r\\ncomputable over arbitrary number of days.\\r\\nSince the main utility of these classifiers is to provide support to staff members\\x92 decisions,\\r\\nin order to consider such classifiers as valid their accuracy clearly should\\r\\nnot be worse than a random guess (50%). Yet, recalling the importance of non\\r\\nover-estimating students\\x92 performance, among the misclassified students the percentage\\r\\nof students predicted with a lower score than the actual one (let us call\\r\\nit safe predictions ratio) should be higher than 50% and as close as possible to\\r\\n100%, meaning in that case that no students were predicted to score higher than in\\r\\nreality. Please notice that by setting these criteria, the worst acceptable classifier\\r\\n(50% of accuracy and 50% of under-estimated students) would already imply that\\r\\nonly 25% of students would be neglected.\\r\\nDG.6. Identification of best predictors of student success\\r\\nBusiness Goals Tackled: BG.2. BG.3.\\r\\nThe predictive model just mentioned can be tuned to produce safe predictions\\r\\nfor ear, but it can also be tuned to maximize accuracy. In that case, the features\\r\\nimportances of the model can give information about the best predictors of student\\r\\nsuccess, which represent an interesting piece of insight for project managers and\\r\\nprofessors.\\r\\n6\\r\\n2 STATE OF THE ART\\r\\nThe goal of the state of the art for this project was to investigate what it has already been\\r\\ndone in the literature and within the institution in terms of Learning Analytics in two main\\r\\nareas: on one hand the definition and monitoring of knowledge and engagement, while\\r\\non the other the creation of a predictive modeling for student success. The following two\\r\\nsections will discuss these two topics more in detail.\\r\\n2.1 Knowledge & Engagement\\r\\nThe first step to make progress was understanding and evaluating the Learning Analytics\\r\\ntools and models in place at the time at IEXL. Although the term model in the field of Data\\r\\nScience is often confused or interpreted as a predictive model, it must be recalled that it\\r\\nrepresents something wider than predictions. In fact, according to the Oxford Dictionary,\\r\\na model is \"a simplified description, especially a mathematical one, of a system or process,\\r\\nto assist calculations and predictions\" [32]. Thus, a model can assist predictions but it does\\r\\nnot necessarily need to predict an outcome.\\r\\nIn fact, this is the case of the model previously in place at IE: it was simply meant to\\r\\ndescribe students learning journey, so to possibly identify those students whose learning\\r\\nexperience was more troubled and intervene. The proposed model described a student\\x92s\\r\\nlearning journey by splitting into two main components: knowledge and engagement.\\r\\nThat is, at every point in time a student can be visualized in a 2D space according to his\\r\\nlevel of knowledge and engagement. Although this representation may seem too simplistic,\\r\\nit is very powerful and extremely easy to read for people like teachers and project\\r\\nmanagers. Yet, its alleged simplicity could be questioned by answering to these two\\r\\nquestions: \"what is knowledge?\" and \"what is engagement?\". In fact, knowledge and\\r\\nengagement are two words that describe very vast concepts, potentially interpretable in\\r\\ndozens or even hundreds of ways. They could be interpreted as simply as grades (knowledge)\\r\\nand attendance (engagement), or in a much more complex way, depending on the\\r\\ntype and number of variables considered.\\r\\nNot very surprisingly, the definition, and the study of these two concepts have been\\r\\nat the center of a good number of publications in the field of Learning Analytics and\\r\\nData-Driven Education.\\r\\n7\\r\\nEngagement\\r\\nBetween the two, engagement is probably the most discussed concept which is more subject\\r\\nto different theorizations and interpretations. This is reflected in the absence of a\\r\\nwidely accepted definition of engagement in Learning Analytics. Relatively recent works\\r\\nhave shown that this confusion has to be attributed to semantic and conceptual inconsistency\\r\\namong the scholars [2][16][19]. On top of that, it must be consider that engagement\\r\\nchanges with respect to the context where it is measured and that there is an incredible\\r\\nvariety of contexts in (higher) education. Can engagement in a traditional course (i.e. provided\\r\\nwith no use of digital means whatsoever) be interpreted and measured in the same\\r\\nway as in a blended or fully on-line course? And even in the case of fully on-line courses,\\r\\nis engagement for MOOCs students the same as for SPOCs students? Clearly, the type of\\r\\ncourse andits modalities define where and how the student engages. Also, is it possible\\r\\nto compare engagement in a traditional course attended regularly by full-time students,\\r\\na MOOC that has no time constraints and a HIOP, a 5-week intensive course designed\\r\\nfor working people? Given these considerations, at this stage the initial questions should\\r\\nbe rephrased to a more appropriate \"what is engagement (and similarly knowledge) in a\\r\\nHIOP?\". The main issue with this question is that literature has either focused on traditional\\r\\ncourses (roughly up to 10 years ago), on blended-learning or on MOOCs. It makes\\r\\nsense that no studies are available for SPOCs, since private institutions generally do not\\r\\nlike to share their data nor results and the small amount of people their projects involve\\r\\noften makes results not statistically significant.\\r\\nA generic recipe for engagement, summarizing its main components and possibly\\r\\napplicable to any context, can be found in Coates\\x92 [11, p.122] who defines it as a combination\\r\\nof\\r\\n1. \"active and collaborative learning,\\r\\n2. participation in challenging academic activities,\\r\\n3. formative communication with academic staff,\\r\\n4. involvement in enriching educational experiences,\\r\\n5. and feeling legitimated and supported by university learning communities.\"\\r\\nMore recently, first [7] and then [16] have elaborated a bit more on this definition and\\r\\nmany previous publications and have offered and interesting and more strict theorization\\r\\nof engagement, dividing it into three components:\\r\\n\\x95 behavioural engagement, which encompasses all kind of measurable actions and\\r\\nhabits towards learning (such as attending classes, submitting assignments, etc.).\\r\\nThis is the type of engagement that is closer to the majority of previous theorizations\\r\\nof students\\x92 engagement, such as the one by Ku et al. (2007) who define\\r\\nit as \"participation in educationally effective practices, both inside and outside the\\r\\nclassroom, which leads to a range of measurable outcomes\" [26]. Also, behavioural\\r\\nengagement seems to reflect points (1) and (2) in Coates\\x92 theorization (\"active [...]\\r\\nlearning\", \"participation\");\\r\\n\\x95 cognitive engagement, which refers to students\\x92 \"psychological investment in learning\"\\r\\nlike \"planning, monitoring and evaluating one\\x92s thinking\" ([16, p.13]). This\\r\\n8\\r\\ntype of engagement is clearly harder to measure because it does not look just at the\\r\\nactions but at their quality, at their thoughtfulness, drawing near points (3) and (4)\\r\\nin Coates\\x92 theory (\"formative communication\" and \"involvement\", which is a more\\r\\naware and thoughtful version of participation) as well as the definition by Hu et al.\\r\\nwho defined engagement as the \"the quality of effort students themselves devote\\r\\nto educationally purposeful activities that contribute directly to desired outcomes\"\\r\\n[22, p.3].\\r\\n\\x95 emotional engagement, which refers to students\\x92 feelings and opinions about the\\r\\nlearning and institutional environment they are in, that is the relationship and the\\r\\nconsideration they have for the institution but also peers, professors and academic\\r\\nstaff in general, expanding point (5) in Coates\\x92 definition of engagement.\\r\\nTo the knowledge of the authors and for the time being these theorizations seem to be\\r\\nthe most complete and the most valid ones. It must not surprise then that these exact same\\r\\nprinciples are the ones used in the National Survey of Student Engagement (NSSE)1, the\\r\\nannual survey conducted among all higher education institutions (both public and private)\\r\\nin US and Canada. As reported in [45], the five axis along which engagement is measured\\r\\nare:\\r\\n\\x96 active learning, that is students\\x92 efforts to actively construct their knowledge;\\r\\n\\x96 academic challenge, that is the extent to which expectations and assessments challenge\\r\\nstudents to learn;\\r\\n\\x96 student and staff interactions, which measures the level and nature of students\\x92\\r\\ncontact with teaching staff;\\r\\n\\x96 enriching educational experiences, that is participation in broadening educational\\r\\nactivities;\\r\\n\\x96 supportive learning environment, which reflect the feelings of legitimation within\\r\\nthe institution.\\r\\nKnowledge\\r\\nLike engagement, also knowledge has been (and still is) victim of semantic inconsistency.\\r\\nIndeed, in almost every piece of literature analysed knowledge is mistaken with outcome,\\r\\nor performance, intended as a grade. Also, knowledge is almost always introduced together\\r\\nand in relation with engagement but it has never been evaluated on its own. While\\r\\nthere are plenty of studies who theorized and defined engagement [11][22][26], investigated\\r\\nits relation with knowledge [6][15][16][34], or tried to predict it from other variables\\r\\n[31][13][12], no studies actually tried to elaborate on the definition of knowledge,\\r\\ndefining its main facets and axis, especially in e-learning contexts.\\r\\nDifferently from the case of engagement, where literature suggested some guidelines\\r\\nto measure it, in the case of knowledge the review of the literature did not provide any help\\r\\nwhatsoever. In all the works reviewed, knowledge is always solely associated to grades.\\r\\nIn fact, this has been true also for the previous analysis done at IE, where knowledge was\\r\\n1http://nsse.indiana.edu/\\r\\n9\\r\\nscored as a simple grades average. According to this author, however, this is a completely\\r\\nerroneous approach and here lays the novelty of the project here presented. In contrasts to\\r\\nother works where knowledge is given as a label, during this project we set a framework to\\r\\nactually label knowledge, identifying its main facets and the importance of each of them.\\r\\nIndeed, grades, albeit meant to measure knowledge, often simply reflect academic performance.\\r\\nAlthough there is undoubtedly a correlation between knowledge and academic\\r\\nperformance (as confirmed in Section 4.3.2), every student has experienced situations\\r\\nwhere he scored more (or less) of what he deserved according to what he knew/studied.\\r\\nAlso, as suggested by [31], in e-learning contexts where some tests can be re-taken or\\r\\nanswers can be given multiple times, a good answer may not necessarily be an indicator\\r\\nof good knowledge if the number of attempts or hints given is also high. For this reason,\\r\\ngrades (or correct answers) cannot be the only element taken into account when it comes\\r\\nto scoring knowledge and this work suggests novel variables and new food for thought.\\r\\n2.2 Predictive Modelling for Student Success\\r\\nPredicting students success is certainly one of the most well-known applications of Learning\\r\\nAnalytics and Educational Data Mining. The reasons of this popularity might be the\\r\\nfact that for most of the institutions, improving students learning performance has been\\r\\nproven to be a key factor in LA adoption [46].\\r\\nDespite its popularity, it has not to be forgot that all the field of LA is very recent\\r\\n(the most widely accepted definition and the first international conference about LA date\\r\\nback to less than 10 years from now [27]) and that the task itself is extremely complex\\r\\nbecause of the irrational nature of learning and the hundreds of different contexts of higher\\r\\neducation. Citing Pepperdine University provost Darryl Tippens: \"higher education is\\r\\nnot a single industry producing a single product, but an extremely varied enterprise, with\\r\\nmore than 4,000 institutions doing different things in different ways, with different ends in\\r\\nmind\" [44]. This is reflected in the incredibly diverse type of contexts, models, algorithms,\\r\\nresults and conclusions reached in the literature in the last 15 years in terms of student\\r\\nsuccess prediction, some of which are summarized in Table 2.1.\\r\\nBecause of this heterogeneity, no conclusions could be drawn about the best methodology\\r\\nnor a valid benchmark for the results could be set. Eventually, most of the effort\\r\\nwas put into compiling a list of most common features used as potential predictors of performance\\r\\nin literature so to get a feeling of which are considered the most useful features\\r\\nand whether there are variables that nobody has ever used for performance prediction.\\r\\nFrom the analysis of such table (see Table 2.2) it emerged that none of the studies actually\\r\\nextracted features from students\\x92 messages using text mining techniques which according\\r\\nto this author is an extremely useful tool to get insights about the quality of students\\x92\\r\\nwriting.\\r\\n10\\r\\nWork Context Problem Methodology & Results\\r\\n[1] fully on-line + blended grades multiple regression: 45-54% var.\\r\\n[15] blended (undergrad) grades correlation: engagement-grades\\r\\n[24] blended (undergrad) risk decision trees 70-90% acc.\\r\\n[25] fully on-line (postgrad) grades regression: 28% var.\\r\\n[28] blended (undergrad) risk SVM: 83% sens., 91% spec.; logistic\\r\\nregression: 88% sens., 90% spec.; Decision\\r\\nTrees: 59% sens., 97% spec.;\\r\\n[30] fully on-line grades; risk linear regression: 33% var.; logistic\\r\\nregression: 75% acc.\\r\\n[33] fully on-line grades regression: 31% var.\\r\\n[51] blended (undergrad) grades regression: 34% var.\\r\\nTable 2.1: Overview of contexts, methodologies and results of researched works in\\r\\nliterature about student performance predictions.\\r\\n11\\r\\nAssignments & Quizzes\\r\\nNumber of assignments submitted [16][30][52][25][24][28][23][13]\\r\\nNumber of quizzes started [16][30][52][25][24][28][13]\\r\\nAverage assignments grades [43][24][28][39][13]\\r\\nNumber of assign. views [30][52][25][13]\\r\\nNumber of quizzes passed [30][52][13][31]\\r\\nNumber of quiz views [52][25][13]\\r\\nNumber of quiz attempts [31]\\r\\nNumber of assign. completed [30]\\r\\nNumber of assign. not delivered [23]\\r\\nDiscussion Forums & Communication\\r\\nNumber of post/comments written [16][30][17][34][51][52][25][3][24][28][23][13]\\r\\nNumber of discussion posts views [33][30][52][25][24][28]\\r\\nNumber of posts created [16][33]\\r\\nNumber of posts/comments updated [25][13]\\r\\nNumber of mails/messages sent to professor [30][51]\\r\\nContents & Resources\\r\\nNumber of content page views [16][30][17][34][51][52][25][3][24][28][23][13]\\r\\nNumber of resources viewed [33][30][52][25][24][28]\\r\\nNumber of course page views [15][52][25][13]\\r\\nNumber of link viewed [30][25][13]\\r\\nNumber of wiki creations/edits [16][33]\\r\\nNumber of wiki views [25][13]\\r\\nNumber of views [15][38]\\r\\nNumber of downloads [51]\\r\\nTime on Platform\\r\\nTotal time on-line (min) [43][30][51][52][38][23][13]\\r\\nTime on contents/resources [33][25][23][31]\\r\\nAverage time per session (min) [17][23][13]\\r\\nIrregularity of study time* [51][13]\\r\\nTime on assignment page [25][31]\\r\\nTime on discussion forum [33][25]\\r\\nTime on writing posts [33][25]\\r\\nTime Diff. due/submission dates [23][31]\\r\\nAverage resource view time [23]\\r\\nTime spent updating posts [25]\\r\\nMax period of inactivity (min) [13]\\r\\nTime until first activity (min) [13]\\r\\nOther\\r\\nNumber of on-line sessions [20][17][51][52][25][24][28][23][35][50][13]\\r\\nTotal number of clicks [43][1][3][18][13]\\r\\nTable 2.2: Variables Used in Literature for Student Success Prediction (adapted from\\r\\n[12])\\r\\n12\\r\\n3 METHODOLOGY\\r\\nIn this chapter, the methodology followed to carry out the whole project and its steps is\\r\\npresented (Section 3.1), together with the theoretical definition of some tools and techniques\\r\\ndeployed during the development (Section 3.2).\\r\\n3.1 CRISP-DM\\r\\nCRISP-DM (CRoss-Industry Standard Process for Data Mining), is one of the most (if\\r\\nnot the most) well-known and used data mining methodologies and process models in\\r\\ndata science [36][14]. Its function is to provide practicioners \"with a complete blueprint\\r\\nfor conducting a data mining project\" [40]. According to its first definition [48], CRISPDM\\r\\nis made of six phases: business understanding, data understanding, data preparation,\\r\\nmodeling, evaluation and deployment. Since these six phases are the same that will be\\r\\nfollowed in the following chapter, here below a short description of each of them is provided.\\r\\n1. Business Understanding\\r\\nIt is the very first and one of the most important steps of the model as it sets the\\r\\nframework and the direction of future work. In this phase the first task is to fully\\r\\nunderstand the goals of the business or client who commissions the work, in this case\\r\\nthe managers at IE Exponential Learning Unit. Some exemplifying questions that need\\r\\nto be which is the context of the project? why do they need to develop a data mining\\r\\nproject? By whom and how is business success assessed? Once business goals have\\r\\nbeen set they must be translated in measurable data mining goals, which will have\\r\\ndifferent success criteria. By defining data mining goals also tools and techniques\\r\\nto be uses should become evident and should be presented as part of a first project\\r\\naction plan, which represents the actual output of this phase. While business and data\\r\\nmining goals have already been outlined in Section 1.2 as part of the Master\\x92s Thesis\\r\\nobjectives, the tools and the techniques used are discussed in this chapter (Section 3.2).\\r\\n2. Data Understanding\\r\\nIn this second phase we could say the action plan just defined \"on-paper\" for the first\\r\\ntime come across the harsh reality of the data. The goal of this phase is to investigate\\r\\nabout which data are actually available, collect them, describe them and assess their\\r\\ncompleteness and/or quality through extensive data exploration. The questions that\\r\\nmust be answered here are: are all the data needed for the project actually present?\\r\\n13\\r\\nIn case they are not, is it possible to augment data, that is extracting the needed information\\r\\nfrom what it is available? Have data the quality needed to proceed further\\r\\nand match the success criteria defined in step 1? In case of negative answer it is wise\\r\\nto re-define the business goals: this explains why business understanding and data\\r\\nunderstanding are connected by a double arrow in the flowchart in Figure 3.1.\\r\\n3. Data Preparation\\r\\nOnce project and data boundaries have been all set, data must be cleaned and prepared\\r\\nfor future modeling. It is one of the longest and most tedious phases of the whole\\r\\nproject: according to a famous poll [5] by Kdnuggets1, around 65% of the data scientist\\r\\ninterviewed revealed that data preparation takes over than 60% of the total time of a\\r\\nproject. As a side note, this figure probably holds for this work, too. At this stage\\r\\nthe datasets defined in step 2 are transformed, integrated and merged to create all the\\r\\nattributes and the records needed. The output of this phase should be a dataset ready\\r\\nto be fed into a machine learning model.\\r\\n4. Modeling\\r\\nIn this phase, different modeling techniques are evaluated, selected and applied. Data\\r\\nare split into training and data sets and test data are fed into the built models. Also,\\r\\nhyper-parameters are fine-tuned to reach optimal values.\\r\\n5. Evaluation\\r\\nAfter having picked up a model, in this stage it must be assessed whether and how\\r\\nthe model fulfils the business goals set in the first place. In case of negative response\\r\\nthis might imply starting everything again from zero, i.e. redefining the goals and the\\r\\naction plan.\\r\\n6. Deployment\\r\\nAt this final stage the trained and evaluated model must become operative: this means\\r\\nthat the customer (in this case teachers and staff) must understand what he got, how to\\r\\nuse it and possibly how to maintain it.\\r\\n3.2 Data Mining Techniques\\r\\nAs part of the methodology used for this project, in the following sections a review of the\\r\\nmajor techniques and algorithms is presented.\\r\\n3.2.1 Data Scaling\\r\\nData scaling (often referred to as features scaling) refers to a set of techniques used to\\r\\ntransform data with the goal of homogenizing the range of its variables or features. Because\\r\\nof this, normalization and standardization are often used interchangeably to indicate\\r\\nscaling, creating some confusion around the topic. Data scaling often uses the\\r\\n1KDnuggets is a renowned and leading on-line data science community. URL:\\r\\nhttps://www.kdnuggets.com/\\r\\n14\\r\\nFigure 3.1: Process diagram showing the relationship between the different phases of\\r\\nCRISP-DM. Source: ResearchGate.net\\r\\ndistributions of the variables to bring them down to common ranges but it is important\\r\\nto underline that the transformations do never change their distribution: if a variable has\\r\\noutliers, it will have the same outliers also after features scaling only with different values\\r\\nassociated to them.\\r\\nFrom a practical point of view, scaling is useful because data often comes with many\\r\\ndifferent features that estimate many different concepts each in different units of measure\\r\\nand this clearly makes them hard to compare. Because of this, scaling is a requirement for\\r\\nmany data mining algorithms, especially those that make use of Euclidean distance (since\\r\\ndistance between two points depends on their absolute coordinates) or that are gradientbased\\r\\n(since the descend is faster when all the variables have homogeneous ranges and\\r\\nvariances).\\r\\nThe most common scaling techniques are two:\\r\\n1. Min-Max scaling: with this approach all the values of a variable are rescaled in\\r\\nthe interval [0; 1] where 0 indicates the minimum value and 1 the maximum. This\\r\\ntechnique is useful when it is important to bring the attention on the best or worst\\r\\nscenario, but it gives almost no information about the distribution and it can be\\r\\neasily misinterpreted. For example, 0 and 1 could indicate both two outliers, and it\\r\\nis not possible to identify a common value for the mean across different variables.\\r\\nGiven a value x from a series S, the scaled value bx will be computed as\\r\\nbx =\\r\\nx ?? min(S)\\r\\nmax(S) ?? min(S)\\r\\n2 [0; 1] 8x 2 S\\r\\n2. Z-scaling (standardization): in this case values are transformed assuming an underlying\\r\\nnormal distribution in the non-scaled data; the scaled variable will have\\r\\n15\\r\\nnull mean and unit variance. This technique allows to easily identify which values\\r\\nare \"normal\" (around the average, thus close to 0), and which values are instead\\r\\noutliers (extreme values, maybe far 3 or 4 standard deviations from the mean). Yet,\\r\\nlike min-max normalization also this method is actually not very resistant to outliers\\r\\nexactly because of the initial assumption on normality. Due to this outliers\\r\\nmay be underestimated and their values may not exactly comparable across different\\r\\nvariables, which makes this technique not very much indicated for non-normal\\r\\ndata. Given a value x from a series S, the scaled value bx will be computed as\\r\\nbx =\\r\\nx ?? mean(S)\\r\\nstd(S)\\r\\n2 [??1;+1] 8x 2 S\\r\\nTogether with these two, a third technique is here considered:\\r\\n3. Inter-quartile scaling (robust scaling): while minimum and maximum values, as\\r\\nwell as mean and standard deviations are very much influenced by the presence of\\r\\nextreme values, quartiles are more resilient and can produce a much more robust\\r\\nscaling, well-indicated whenever data contain outliers or normality is not guaranteed.\\r\\nBecause of this, robust scaling is the technique that was ultimately chosen\\r\\nto normalize the features datasets used for modeling (details are discussed in Section\\r\\n4.2, see Figure 4.12 for a comparison). Given a value x from a series S, the\\r\\nscaled value bx will be computed as\\r\\nbx =\\r\\nx ?? Q2(S)\\r\\nQ3(S) ?? Q1(S)\\r\\n=\\r\\nx ?? median(S)\\r\\nIQR(S)\\r\\n2 [??1;+1] 8x 2 S\\r\\n3.2.2 Dimensionality Reduction\\r\\nDimensionality reduction is an expression to indicate another set of techniques used, as\\r\\nthe name says, to reduce the number of dimensions (i.e. features, variables) of a dataset. It\\r\\nis generally performed after features extraction. The reasons that motivate dimensionality\\r\\nreduction can be various, yet it is generally used to identify and get rid of uninformative\\r\\nvariables and more in general to keep data as dense as possible, that is containing the\\r\\ngreatest amount of information in the smallest number of dimensions possible. This is\\r\\noften done to forestall the so-called \"curse of dimensionality\" [4], that in machine learning\\r\\ncould be described as the inverse relationship that arises between model value and number\\r\\nof dimensions when the latter grows above a certain threshold (see Figure 3.2).\\r\\nThe simplest way to reduce the number of features is to filter some out. The most\\r\\ncommon ways to filter the variables are removing high-correlated features as well as lowvariance\\r\\nones.\\r\\n1. High-correlation filter: highly correlated features are likely to intrinsically contain\\r\\nthe same amount of information; thus, for each pair of correlated features one may\\r\\nbe removed. For example, if to predict the performance of a student one feature tells\\r\\nhis activity time expressed in number of days, knowing it in hours would not add\\r\\nany additional information, so one of the two variables can be discarded without any\\r\\n(big) loss of information. Although there is not a predefined value for the threshold\\r\\nto use, a (absolute) correlation of 0.7 generally indicates a strong linear relationship.\\r\\n16\\r\\nFigure 3.2: General relationship between number of dimensions and model value in\\r\\nclassification scenarios, often known as curse of dimensionality. The value (e.g.\\r\\naccuracy) grows drastically up to a certain number of features after which data start to\\r\\nget too sparse to be learned by the model.\\r\\n2. Low-Variance filter: if it is true that each feature must be as much informative as\\r\\npossible, features with low variance are more likely to be discarded. To understand\\r\\nwhy, think about the extreme case of features with variance equal to 0: null variance\\r\\nmeans that a variable is constant, which in turn implies that the variable cannot not\\r\\nbe of any help in distinguishing between two or more outcomes. Following this\\r\\nline of reasoning low-variance are considered uninformative and thus should be\\r\\ndiscarded. Also in this case there is not a default value for the variance to indicate\\r\\nthat a variable is uninformative. What must be taken into account, though, is that\\r\\nvariance of a variable is proportional to its numerical range, so data must be (minmax)\\r\\nnormalized before computing and evaluating the variance. As a general rule,\\r\\nfor the purpose of this project, 0.02 was considered to be a fair value.\\r\\nIt is also worth mentioning two other very popular techniques for dimensionality reduction:\\r\\nPrincipal Component Analysis (PCA) and (Backward/Forward) Features Elimination.\\r\\nThe former reduces the number of dimensions by iteratively looking for the directions\\r\\n(technically, eigenvectors) along which most of the variance of the data is explained, and\\r\\nthen framing the data into the new set of axis. Even if it can be extremely powerful, PCA\\r\\nwas not used in this scenario because the eigenvectors (that is the dimensions of the new\\r\\nspace which data are shaped into) are linear combinations of the original features and are\\r\\nvery hard to interpret for a teacher or a member of the staff, who in turn would like to be\\r\\nable to associate an importance to a very precise variable/dimension.\\r\\nThe latter sets up a classification scenario, iteratively removes (adds) features from the\\r\\ntraining set and records the accuracy loss (increase) got for any experiment so to identify\\r\\nthe features that have less influence on final performance and should be removed. In this\\r\\ncase this technique was not used because of the small amount of data available.\\r\\n17\\r\\n3.2.3 Resampling & Synthetic Data Generation\\r\\nIn case of supervised learning, and in the specific case of classification, a classifier learns\\r\\nto classify samples as belonging to one or another label. The idea is that by observing\\r\\nenough samples of a given label, the model should be able to generalize its features and\\r\\nidentify its main traits so to eventually classify with the same label every other unseen\\r\\nsample that matches those criteria. Hence, it is of supreme importance that the model gets\\r\\nto see enough samples for each label that wants to be predicted, otherwise its \"judgement\"\\r\\nwill be distorted. Put into technical words, this problem is called class imbalance. Class\\r\\nimbalance represents a very serious issue that can impact negatively on the training of a\\r\\nmodel and consequently on its final performance, too. Among the most common solutions\\r\\nto this problem there are resampling and synthetic data generation.\\r\\nThe main resampling techniques are two: over-sampling and under-sampling data.\\r\\n\\x95 Over-sampling: in this case, a minority class is balanced with the other(s) by randomly\\r\\nre-sampling some of its observations. This is equivalent to repeating some\\r\\nsamples. Clearly, this procedure has the advantage that no data points are removed,\\r\\non the contrary the number of samples grows; on the other hand however, repeating\\r\\na sample multiple times introduces the risk of overfitting, especially in those cases\\r\\nof strong imbalance.\\r\\n\\x95 Under-sampling: contrary to the case of over-sampling, here the balance is reached\\r\\nby simply down-sizing the majority class(es) to match the size of the minority class.\\r\\nThis technique avoids to repeat samples, but it is also true that by omitting samples\\r\\nfrom the training process also the quality of training is degraded because many\\r\\ninformative observation may have been neglected. Also, although no mentions of\\r\\ndata quantity issues for this project have been made yet, it seems pretty obvious that\\r\\nunder-sampling is not a viable solution in cases of little data available.\\r\\nTaking into consideration the strong downsides of both these techniques, they have\\r\\nbeen discarded in favour of more recent ones. Differently from resampling techniques,\\r\\nmore recent solutions allow to balance classes by over-sampling with synthetic data.\\r\\nAmong these techniques, two of the most popular ones are certainly SMOTE (Synthetic\\r\\nMinority Over-sampling Technique [8]) and ADASYN (ADAptive SYNthetic [21]).\\r\\nWhile the \"naive\" over-sampler generates new samples by simply duplicating existing\\r\\ndata points, SMOTE and ADASYN generate new samples by interpolation. The generic\\r\\napproach is the same is the following: pick a sample of the minority class to be balanced,\\r\\nidentify its k nearest neighbors (using kNN) within the same minority class, then pick\\r\\npairs of points from the cluster and generate new synthetic points at a random point on\\r\\nthe line that connects them. Yet, the approach to select the pair between which to interpolate\\r\\na third point differ. In fact, ADASYN uses a density function to create more\\r\\npoints around the samples of the minority class that are more difficult to learn (this is\\r\\ndone computing per each sample in the minority class another metric that is equal to the\\r\\npercentage of its neighbors that belong to the majority class); on the other hand, the basic\\r\\nimplementation of SMOTE does not make any difference at all.\\r\\n18\\r\\nBy definition, ADASYN creates new samples interpolating outliers only; on the other\\r\\nhand, SMOTE does not make any distinction so it may generate a new sample by interpolating\\r\\nan inlier and an outlier, so each of these approaches has its own pros and cons.\\r\\n3.2.4 Cross-Validation\\r\\nCross-validation is a technique used for model selection. The term validation is due to the\\r\\nfact that this technique helps proving the generalizability and (consequently) the validity\\r\\nof the results. To do this, the intuition is that only by repeating an experiment multiple\\r\\ntimes it is possible to really evaluate the results; on the contrary, if a model is only trained\\r\\nand tested once, whatever good or bad score it may get it might simply be result of good\\r\\nor bad \"luck\" in the composition of training and testing sets. At this point, it is worth\\r\\nrecalling that these two datasets must be completely distinct as it is not possible to test a\\r\\nmodel on samples that were already observed during training: staying on topic, it is not\\r\\npossible to properly evaluate a student by testing him on exercises he already solved at\\r\\nhome. Here-hence, the need to split data into always different training and testing sets in\\r\\neach run (i.e. experiment), which is referred to as crossing.\\r\\nThe strategies according to which data are split define different types of cross-validation.\\r\\nThe ones considered in the context of this project are two:\\r\\n1. k-fold: in k-fold cross-validation, data are firstly split into k equally sized parts,\\r\\nthen k experiments are run every time using k ?? 1 folds (i.e. parts) for training and\\r\\n1 fold for testing. This procedures ensures that each data point is used at least once\\r\\nboth for training and for testing and it is most probably the most common crossvalidation\\r\\ntechnique used in data science. Recalling the issues with class imbalance\\r\\njust discussed, it is worth to mention that when data are split, the resulting datasets\\r\\nmight be suffer from class imbalance. Because of this k-fold cross-validation often\\r\\nuses a splitting technique called stratified sampling which ensures the same proportion\\r\\nof samples per each label both in training and testing set.\\r\\n2. Leave-One-Out: it is actually a special case of k-fold cross-validation, more specifically\\r\\nthe case in which k is equal to the number of samples in the dataset considered.\\r\\nThis procedure leaves the maximum number of samples possible available for\\r\\ntraining at each run, thus maximizing the information available for training. On the\\r\\nother hand, it is much more time and resource consuming, so it is well indicated in\\r\\nthose cases where little data are available.\\r\\n3.2.5 Clustering\\r\\nAs specified in the data mining goals in Section 1.2, clustering is a technique that will be\\r\\nused to identify groups of students. The algorithms tested are the following:\\r\\n\\x95 k-Means\\r\\nk-Means is arguably the simplest clustering algorithm. The algorithm starts selecting\\r\\nk points (called centroids) at random, then it assigns every remaining point to\\r\\n19\\r\\nthe closest2 centroid available and finally it updates the centroid to be the points\\r\\nwhose coordinates are the average values of the coordinates of all the points belonging\\r\\nto the same cluster. These three steps are repeated until no point changes\\r\\ncentroid.\\r\\n\\x95 Hierarchical (Agglomerative)\\r\\nHierarchical clustering is an expression that indicates a family of algorithms rather\\r\\nthan a specific one. Among these algorithms, some build clusters by merging subclusters\\r\\n(agglomerative), others by splitting them (divisive). Agglomerative clustering\\r\\nare considered by far the most common ones [42, p.515]. Here the approach\\r\\nis bottom-up: start considering each data point as a separate cluster, then iteratively\\r\\nmerge together the two closest clusters until the specified number of clusters is\\r\\nreached.\\r\\nTo compare the quality of the clusters and find the best number of clusters the main\\r\\nmetric used was the silhouette score. Silhouette score is a measure that tries to account\\r\\nfor two different factors: cohesion and separation. The intuition is that the quality of a\\r\\ncluster grows as the distance between its members is reduced and the distance with the\\r\\nother clusters is increased. Let C be the set of clusters and xA be a data point belonging\\r\\nto cluster A 2 C. It silhouette score will be computed as follows:\\r\\n1. compute the average distance of the object from all the other points in the same\\r\\ncluster \\rxA:\\r\\n\\rxA = mean\\r\\na2A\\r\\njjxA ?? ajj\\r\\n2. for any cluster C 2 C ?? fAg (that is all the clusters except the one to which the\\r\\npoint considered belongs), let \\x12xA;C be the average distance of the point to all the\\r\\npoints of the cluster:\\r\\n\\x12xA;C = mean\\r\\nc2C\\r\\njjxA ?? cjj\\r\\n3. let \\x12xA be the minimum distances across all clusters:\\r\\n\\x12xA = min\\r\\nC2C??fAg\\r\\n\\x12xA;C\\r\\n4. finally compute the silhouette score \\x1bx as follows:\\r\\n\\x1bxA =\\r\\n\\x12xA ?? \\rxA\\r\\nmax(\\rxA; \\x12xA)\\r\\n2 [??1; 1] 8x 2 A\\r\\nIt follows that if the cohesion is maximum (\\r = 0) also silhouette is maximum and\\r\\nis equal to 1; on the other hand if the point is closer to other clusters than to its \"peers\"\\r\\n(\\r < \\x12), then silhouette has negative values. Hence, the best value for silhouette is 1,\\r\\nwhile the worst is ??1. Still, this measures regards one single point. To have a measure of\\r\\nhow good a whole clustering is, this value is computer for all the data points and averaged.\\r\\n\\x1b =\\r\\nP\\r\\nC2C\\r\\nP\\r\\nP xC2C \\x1bxC\\r\\nC2C jCj\\r\\n2Please notice that \"closest\" implies the adoption of a distance metric. Unless otherwise specified,\\r\\nEuclidean distance is the one considered.\\r\\n20\\r\\n3.2.6 Classification\\r\\nFor the classification tasks planned to predict students\\x92 final grades, three algorithms have\\r\\nbeen considered: Random Forest, (Decision Tree) Gradient Boosting and eXtreme (Decision\\r\\nTree) Gradient Boosting. Although they all differ somehow, they are all ensemble\\r\\nmethods (that is, they are combination of multiple algorithm as explained shortly) and\\r\\nthey are all decision-tree based. Because of this, it seems wise to start first with a little\\r\\ndescription of decision trees and move later to the algorithms mentioned.\\r\\n\\x95 Decision Tree\\r\\nA decision tree is a supervised machine learning algorithm used both for classification\\r\\nand regression problem. Yet, in this case only classification trees are considered.\\r\\nTo classify data points, a decision tree repeatedly splits data into two or more groups\\r\\nwith the goal of maximizing their \"purity\", that is trying to create groups containing\\r\\npoints of one class only. More formally, the splitting strategy in a decision tree\\r\\nis explained in terms of entropy and information gain. Entropy is a measure of\\r\\nuncertainty in the data, the opposite of what has been previously called \"purity\".\\r\\nGiven a set of outcomes (i.e. classes) C with each possible outcome c 2 C having\\r\\nprobability pc, the entropy E of the set of data S is given by the formula\\r\\nE(S) = ??\\r\\nX\\r\\nc2C\\r\\npc \\x01 log2pc 2 [0; 1]\\r\\nwhere 1 indicates maximum uncertainty (50%-50%) and 0 minimum uncertainty\\r\\n(only one outcome present). Clearly, the goal of classification is to reach states\\r\\nwith null entropy.\\r\\nOn the other hand, information Gain is the reduction of entropy in a set of data after\\r\\nsplitting it according to a given attribute. When splitting, the entropy of the split is\\r\\ncomputed by summing the entropies of all the subsets generated by the split, each\\r\\nweighted by its probability\\r\\nE(S;A) =\\r\\nX\\r\\na2A\\r\\npc \\x01 E(a) 2 [0; 1]\\r\\nwhere A is the set of subsets generated by the split according to the selected attribute.\\r\\nTherefore, the information gain of S according to the attribute that generates\\r\\nthe set of splits A is given by\\r\\nIG(S;A) = E(S) ?? E(S;A)\\r\\nSo the training algorithm of the decision tree starts with all data in one group,\\r\\nthen iteratively and recursively creates new branches splitting data according to the\\r\\nattribute that maximizes the information gain, theoretically until all the reached\\r\\nstates have null entropy. To classify, new data points are run through the tree,\\r\\n21\\r\\nfollowing the splits that match their attributes until they reach a leaf, that is the\\r\\nfinal class.\\r\\nTo conclude, it must be noted that training a decision tree to reach all states with null\\r\\nentropy often leads to the generation of a very complex tree that overfits training\\r\\ndata and performs poorly on new unseen data. Hence, trees are often \"pruned\", that\\r\\nis branches are stopped from growing although all the data in the last leaf has not\\r\\nbeen perfectly binned yet (i.e. entropy is still greater than zero).\\r\\nAfter this brief introduction about decision trees, the actual algorithms considered are\\r\\nthe following:\\r\\n\\x95 Random Forest\\r\\nAs the term forest suggests, this algorithm can be thought as an ensemble of (decision)\\r\\ntrees. On the other hand, the term random here refers to the fact that each tree\\r\\nadded to the forest is built starting from a set of data that is obtained by randomly\\r\\nsampling rows (with replacement, what is called bootstrap sampling) and columns\\r\\n(without replacement) of the original dataset. With this method, same data points\\r\\nare generally used multiple times in all differently combined datasets to train many\\r\\nindependent trees, acting like a sort of cross-validation. This is a reason why Random\\r\\nForests are generally considered very resilient to over-fitting and very robust\\r\\nto any type of data. To take decisions (i.e. to classify), random forest uses what it\\r\\nis called majority vote: it takes the output of each tree in the forest and then returns\\r\\nthe \"most voted\" output, that is the class which most of the trees agree on.\\r\\nThis type of practice, that is aggregating results of many independent models, is\\r\\noften referred to as bootstrap aggregation or bagging. Taking these aspects into\\r\\nconsideration it is clear why growing the number of trees in the forest (thus the\\r\\nnumber of different \"experiments\") does not over-fit data; on the contrary, it is\\r\\ngenerally considered a good practice to increase model accuracy (and performance\\r\\nin general).\\r\\n\\x95 (Decision Trees) Gradient Boosting\\r\\nOften presented in juxtaposition to bagging, boosting \"refers to a family of algorithms\\r\\nthat are able to convert weak learners to strong learners\" [53, p.23], where\\r\\nweak learner (in the context of classification) stands for a classifier that is just\\r\\nslightly better than random guess. It is clear then that boosting is a technique that\\r\\ncan be applied to any algorithm; still, most of the times it refers to decision trees.\\r\\nAmong the boosting techniques, gradient boosting is surely one of the most famous\\r\\nones. The intuition behind Gradient Boosting is that subsequent learners can learn\\r\\nfrom the errors of the previous ones. In practice, this implies that learners are no\\r\\nmore independent such in the case of bagging and also that data are no sampled\\r\\nrandomly any more as sampling now promotes those data points that have been\\r\\nmisclassified by the previous learner(s). This approach highlights both a weak and\\r\\na strong point of Gradient Boosting: since each new tree aims at reducing the error\\r\\nof its \"father\", by adding too many trees (thus, by doing too many iterations)\\r\\n22\\r\\nthe algorithm may start hunting for noise, causing serious over-fitting problems;\\r\\non the other hand, for the same reason Gradient Boosting can help solving cases\\r\\nwhere model under-fits the data, that is bias-related problems. In any case this\\r\\nimplies that Gradient Boosting, as opposed to Random Forest, needs scrupulous\\r\\nhyper-parameters fine-tuning.\\r\\n\\x95 XGBoost: eXtreme (Decision Trees) Gradient Boosting\\r\\neXtreme Gradient Boosting (mostly known as XGBoost) is \"an optimized distributed\\r\\ngradient boosting library\" which \"implements machine learning algorithms\\r\\nunder the Gradient Boosting framework\" [49]. According to its author, eXtreme\\r\\nGradient Boosting could be simply defined as a more \"regularized gradient boosting\",\\r\\nwhile its name actually \"refers to the engineering goal to push the limit of\\r\\ncomputations resources for boosted tree algorithms\" [9]. XGBoost has been taken\\r\\ninto consideration for this work together with its own more classical definition not\\r\\nonly because of its novelty (its reference paper dates back to 2016 [10]) but also\\r\\nbecause of its increasing popularity and impressive results: interestingly enough,\\r\\naccording to [47], XGBoost library \"is taking over practically every competition\\r\\nfor structured data\" hosted on Kaggle3, one of the largest and most active communities\\r\\nfor data scientists worldwide, which makes it a very strong candidate.\\r\\n3https://www.kaggle.com/\\r\\n23\\r\\n\\r\\n4 DEVELOPMENT\\r\\nIn this chapter, the CRISP DM stages outlined in Chapter 3 are now extensively discussed.\\r\\nSince business understanding has already been covered (first in Section 1.2 and then in\\r\\nSection 3.2), here only the remaining phases will be presented: first data unterstanding\\r\\n(Section 4.1) and data preparation (Section 4.2); then modeling and evaluation will be\\r\\npresented together (Section 4.3) with an internal distinction between the model designed\\r\\nfor the definition of key knowledge and engagement indicators (Section 4.3.2) and the\\r\\nmodel for predicting students\\x92 success (Section 4.3.3); finally, the chapter closes with a\\r\\nsmall digression about the final deployment of the models (Section 4.4).\\r\\n4.1 Data Understanding\\r\\nThe first step required for the advancements commented in Section 2.1 was the automation\\r\\nof information retrieval. Given that all the HIOPS are currently provided on Canvas\\r\\nLMS1, this meant exploring and mastering the use and knowledge of the Canvas LMS\\r\\nAPI2. In this section, a first description and exploration of the raw data available in Canvas\\r\\nis presented (Section 4.1.1). The idea of this section is to document which were\\r\\nthe data available, how they were inspected, cleaned and in some cases filtered or augmented\\r\\nin order to have all the important elements needed for the further steps of the\\r\\nproject. Also, it is worth recalling that data itself, through visualization, can already provide\\r\\nuseful insight about the courses and at least partially fulfil some of the goals defined\\r\\nin Chapter 1. Because of this, some relevant visualizations will be presented together with\\r\\ntheir corresponding datasets by means of example.\\r\\n4.1.1 Data Collection\\r\\nGiven that the information and the number of datasets that is possible to download through\\r\\nCanvas API is almost endless, the first step in data collection was to explore all the possible\\r\\ncalls and define which data were going to be collected. In the end, the raw datasets\\r\\nselected were the ones showed in Table 4.1) and they could be divided into two macroareas:\\r\\n1Learning Management System | LMS | Canvas by Instructure. https://www.canvaslms.com/. Last access:\\r\\n27th July 2018.\\r\\n2Canvas LMS REST API Documentation. https://canvas.instructure.com/doc/api/. Last access: 27th\\r\\nJuly 2018.\\r\\n25\\r\\n1. course structure: this group includes all the datasets with information relative to\\r\\nthe courses and their contents, like the modules and the items contained in each\\r\\nmodel, the assignments, the threads opened in the discussion forums, etc.;\\r\\n2. users\\x92 activity: it includes all the datasets that contain information about the interactions\\r\\nof the students with the course and the LMS in general. In this category\\r\\nfall the logs with users authentication events to the LMS (logins/logouts) or the\\r\\nlogs with their page views (unfortunately no clicks log is available), as well as all\\r\\nthe messages posted in the discussion forums, read and liked posts, etc.. These\\r\\ndatasets must be first cleaned from missing, redundant or irrelevant information as\\r\\nfor the others, but they can also be explored to extract useful insight about students\\x92\\r\\nbehaviour.\\r\\nType Name Content\\r\\nCourse\\r\\nStructure\\r\\nAssignments Details about all exams, i.e. assignments\\r\\nand quizzes\\r\\nDiscussion Topics Details about the threads opened in the\\r\\ndiscussion forum\\r\\nItems Course contents, such as video lectures,\\r\\nreadings and assignments or quizzes\\r\\nQuizzes Questions Statistics about the questions present in\\r\\nall the quizzes of the course\\r\\nUsers\\x92\\r\\nActivity\\r\\nAuthentications Log of authentication events (logins and\\r\\nlogouts) performed by users.\\r\\nPage Views Log of pages visited by the course users\\r\\nPeer Reviews Details about peer reviews\\r\\nPeer Reviews Comments Comments written by course users in the\\r\\npeer reviews section\\r\\nDiscussions Details about all threads and messages\\r\\nposted by course students\\r\\nSubmissions Details about all assignment and quiz\\r\\nsubmissions by course students\\r\\nTable 4.1: List of raw datasets of interest accessible through Canvas API.\\r\\nGeneral Issues & Limitations\\r\\nAlthough the REST API provides easy access to information, it does not mean it also\\r\\nprovides easily accessible data. Therefore, here some issues and limitations of Canvas\\r\\nAPI will be briefly discussed.\\r\\n26\\r\\nThe first issue is related to the type of data returned by the API. Indeed, calls to\\r\\nCanvas API return JSON objects, which could be defined as semi-stuctured data: although\\r\\nJSON data have a well-defined structure, it is clearly not optimal for transformations and\\r\\nsearches. Therefore the structure of all the results returned by each API call had to be first\\r\\nunderstood and then converted to more appropriate formats such as Comma Separated\\r\\nValues.\\r\\nThe second issue has to do with API limitations, in particular the limitation in the\\r\\nmaximum number of results (read \"rows\") returnable in one call which is set to one hundred\\r\\n(100) and cannot be raised. Because of this it was necessary to create wrappers for\\r\\neach API call that would repeat the call as long as there are new results. The logic behind\\r\\nit is really simple (see Algorithm 1): make the request, if the length of the returned results\\r\\nis less than 100 then it means that there are no more results, otherwise the request has to\\r\\nbe performed again incrementing the number of the page requested until there will be less\\r\\nthan 100 results, each time appending the new results to the old ones.\\r\\nAlgorithm 1 Wrapper for Http GET requests to Canvas API.\\r\\n1: procedure GET_RESULTS(REQUEST_URL, API_TOKEN)\\r\\n2: results = None\\r\\n3: page = 1\\r\\n4: while True do\\r\\n5: new_results = http_get_request(request_url; page; canvas_api_token)\\r\\n6: results = append(results; new_results)\\r\\n7: if len(new_results) < 100 then return results\\r\\n8: page + +\\r\\nFinally, it must be consider that Canvas API often provides information only for one\\r\\ncourse or one topic or one student at a time, so many datasets must be composed by calling\\r\\nthe corresponding function multiple times and appending the results.\\r\\nAutomation\\r\\nEach API call returns a JSON object that is converted and stored as a CSV. The idea here\\r\\nis that it is possible to construct and look at a course as a set of CSVs (from now on\\r\\ninterchangeably referenced to as datasets or dataframes, from the popular Python pandas3\\r\\ndata structure), plus some basic attributes such as the name, the teaching language, the\\r\\nstarting and end date.\\r\\nThe datasets required to put a course together could be divided into two types: the\\r\\nones that are automatically produced (through Canvas LMS API calls) and the ones that\\r\\nare required, i.e. that must be filled in by course staff.\\r\\nAll the datasets are stored on an Amazon S34 server. While the required datasets are\\r\\nmanually uploaded to S3 when necessary, the rest of the datasets are downloaded thanks\\r\\n3\"pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures\\r\\nand data analysis tools for the Python programming language\" - from https://pandas.pydata.org/\\r\\n4Amazon S3 provides a remote (cloud) storage service. https://aws.amazon.com/s3/\\r\\n27\\r\\nto a script on an Amazon S2 node that is automatically run daily.\\r\\nFigure 4.1: UML Chart and Architecture of the software tool developed for automated\\r\\ndata collection, storage and processing.\\r\\n4.1.2 Data Description\\r\\nIn this section, all the raw data accessible through Canvas will be described and the details\\r\\nof how they were handled and cleaned will be given. Considering the large amount of\\r\\ndatasets, it is impossible to propose an exhaustive description/analysis of all of them.\\r\\nAlso, it is important to underline that most of these datasets come with many irrelevant\\r\\nor unusable fields that were discarded beforehand and thus are not here described. As a\\r\\nrule, the field dropped were those which had either 100% of missing values or had only\\r\\none unique value (constants), as well those which where redundant (duplicates) or simply\\r\\nwere not necessary for further use. Because of this, the list of attributes proposed in the\\r\\nfollowings pages do not have to be considered the complete lists of attributes but rather\\r\\nthe list of attributes considered useful for the project. Still, the reader may notice that in\\r\\nthe different tables with fields descriptions, some fields are presented separately and with\\r\\nitalic font: these are all the fields that have been added to or extracted from raw data.\\r\\nTo keep things as clear as possible, the datasets will be discussed following the order\\r\\nand the structure proposed in the previous section (Section 4.1.1).\\r\\n28\\r\\nCourse Structure\\r\\nThese datasets do not provide any insight in the way students access and use the platform\\r\\nand the content, yet it is important to investigate whether the structures of the several\\r\\ncourses are actually comparable. As this data define the general framework, it must be\\r\\ncleaned first.\\r\\n\\x95 Assignments\\r\\nAssignments dataset contains information about all kinds of tests, that is proper assignments\\r\\n(that is graded homework) and quizzes (online tests) and it is one of the most\\r\\ncomplex ones. Interestingly, also final satisfaction surveys are considered assignments,\\r\\nbut they have been removed because not actually graded.\\r\\nIn Table 4.2 it is possible to see that there are very few missing values and they interest\\r\\nonly two fields. In the case of content_id it can be shown that values are missing only\\r\\nin case of proper assignments: as the name says, content_id strictly depends on the\\r\\ncontent considered; quizzes are a special subset of assignments and can be considered\\r\\nas a different content from proper assignments, so they have a different content_id. This\\r\\nfield can be easily filled using the assignment_id. On the other hand, the field unlock_at\\r\\nis only missing just for the surveys just mentioned, so these missing values have been\\r\\nremoved by removing the entire corresponding rows.\\r\\nLooking at the dates, there are 2 main fields: unlock_at and due_at. The former indicates\\r\\nthe date and time when the assignment was first made available for students (from\\r\\nnow on called publication date) while the latter specifies the last time available for the\\r\\nassignment to be submitted (due date). By inspecting the second one (which has no\\r\\nmissing values) it emerges that there are two missing assignments: A1.1 in course 31\\r\\nand Q2.1 in course 20. Looking at the first one, instead, it is possible to see that only\\r\\none publication date is actually missing: A1.1 in course 43. Interestingly, looking at\\r\\nthe days of the week Table 4.3, it is possible to observe an interesting pattern: with\\r\\nthe exception of the first two courses, the others always publish all exams on Fridays,\\r\\nmeaning that it makes very much sense to fill that only missing date with the Friday\\r\\nof the corresponding week (2018-04-13); also, the value of A1.1 for course 15 clearly\\r\\nmakes no sense and has to be overwritten: using the value for A1.2 and Q1.1 seems a\\r\\nmuch better choice. This last table highlights another very important point: courses 15\\r\\nand 20, the oldest ones, follow two separate structures which are different compared to\\r\\nthe last four (31, 35, 43 and 47), which on the other hand seem to be more standardized.\\r\\nSpeaking about data consistency, an important aspect to validate here is whether assignments\\r\\nhave consistent names across courses (so that they can be compared) as well as\\r\\nthe same order in the course structure. About the names of the assignments, in theory\\r\\nthey should be the same across all the intakes with the same language because they are\\r\\n\"copies\" of the same course; also, there should be only 6-7 different assignments names\\r\\nper language: the initial one to assess students knowledge before the course, then one\\r\\nfor each week and (optionally) a final survey. Yet, looking at the possible values of the\\r\\nname column, there are 34 different values. Table 4.4 shows all the different names\\r\\npresent in the dataset and how they have been changed to a new name, more suitable\\r\\n29\\r\\nField Name Type Description Nulls (%)\\r\\nassignment_id id unique id for the assignment \\x97\\r\\nname string name of the assignment \\x97\\r\\nposition int the position of the assignment in\\r\\nthe course structure\\r\\n\\x97\\r\\nassignment_url string Canvas URL to access to the page\\r\\nof the assignment\\r\\n\\x97\\r\\nis_quiz_assignment bool whether the assignment is quiz or\\r\\nnot\\r\\n\\x97\\r\\nomit_from_final_grade bool whether the assignment accounts\\r\\nfor the final grade or not\\r\\n\\x97\\r\\nunlock_at datetime when the assignment was published\\r\\nonline\\r\\n1.56%\\r\\ncreated_at datetime when the assignment was created \\x97\\r\\ndue_at datetime when the assignment is officially\\r\\ndue\\r\\n\\x97\\r\\ngroup_id float id of the group the assignment belongs\\r\\nto\\r\\n\\x97\\r\\ngroup_weight float weight of the group the assignment\\r\\nbelongs to\\r\\n\\x97\\r\\ngroup_name string name of the group the assignment\\r\\nbelongs to\\r\\n\\x97\\r\\ngroup_position int position of the group of the assignment\\r\\n\\x97\\r\\npoints_possible float maximum points that is possible to\\r\\nscore\\r\\n\\x97\\r\\ncourse_id int id of the course the assignment\\r\\nrefers to\\r\\n\\x97\\r\\ncontent_id float id of the assignment as a course\\r\\ncontent\\r\\n64.06%\\r\\ncomparable_name string standardized name \\x97\\r\\nTable 4.2: [Assignments] Fields description.\\r\\n30\\r\\nfor comparison.\\r\\nThe second important issue has to do with the positioning of the assignments. In fact,\\r\\nalthough there are two attributes meant to give information about the position of the\\r\\nassignments (group_position and position), they are unusable because they happen to\\r\\nreflect the position within the structure proposed on the course page rather that the actual\\r\\nposition within the course structure. A more valid indexing could be obtained by\\r\\nsimply considering the first number of the new name and interpreting it as the week of\\r\\nthe assignment.\\r\\nAssignments & Quizzes Publication Dates\\r\\nweek Week 0 Week 1 Week 2 Week 3 Week 4-5\\r\\nid A0.1 A1.1 A1.2 Q1.1 A2.1 Q2.1 A3.1 Q3.1 A4.1 Q4.1 A5.1\\r\\n15\\r\\n2017-10-16 2017-05-18   2017-10-18 2017-10-25 2017-11-01 2017-11-08\\r\\nMON THU  WED WED WED WED\\r\\n20\\r\\n2017-09-03 2017-09-08 2017-09-14 2017-09-11 2017-09-18 2017-09-25 2017-10-02\\r\\nSUN FRI THU MON MON MON MON\\r\\n31\\r\\n2018-01-15 2018-01-19 2018-01-26 2018-02-02 2018-02-09\\r\\nMON FRI FRI FRI FRI\\r\\n35\\r\\n2018-02-14 2018-02-16 2018-02-23 2018-03-02 2018-03-09\\r\\nWED FRI FRI FRI FRI\\r\\n43\\r\\n2018-04-09 NaT  2018-04-13 2018-04-20 2018-04-27 2018-05-04\\r\\nMON NaT  FRI FRI FRI FRI\\r\\n47\\r\\n2018-05-16 2018-05-18! 2018-02-23 2018-05-25 2018-06-01 2018-06-08\\r\\nWED FRI! FRI FRI FRI FRI\\r\\nTable 4.3: [Assignments] Publication dates of each assignment, together with the\\r\\ncorresponding day of the week. Inconsistencies are highlighted in bold text. Arrows\\r\\ngoing from one cell to another indicate which values have been used to replace\\r\\ninconsistent values.\\r\\nOnce assignments names and positions have been standardized, another point to discuss\\r\\nis the importance of the assignment for the final grade. This information is provided by\\r\\na pool of attributes: group_id, group_name, group_weight and omit_from_final_grade.\\r\\nIn Table 4.6 it is possible to see how the initial assignment belongs to different assignment\\r\\ngroups across different courses and Q1.1 unexpectedly accounts for Module 2 and\\r\\nnot Module 1 in course 20, but also how same group assignments have different weights\\r\\nin different courses. Moreover, inspecting omit_from_final_grade it appears that the\\r\\nsame assignments do not always count for the final grade. After having discussed this\\r\\nissue with the teachers it was possible to find out that the assignment A0.1 never counted\\r\\nand should never count, while the assignments A1.1, A2.1 and A3.1 have been actually\\r\\ntaken into account only since the course 31. Nonetheless, even though these assignment\\r\\nmay have not been taken into account in the past, they provide useful information about\\r\\nthe performance trend of the students and have been considered anyway. Differently,\\r\\nall the initial assignments have been overwritten so to belong to the module 0 and to\\r\\nhave weight equal to zero. More in general, since all these small defects in data affect\\r\\nautomation, it was decided to overwrite the assignment groups and to rather consider\\r\\nassignments and quizzes of the same week to belong to the same group. Also, to homogenize\\r\\nthe final students\\x92 scores, the weights have been overwritten so to be always\\r\\nequal to 25%.\\r\\n31\\r\\nSpanish Name New Name English Name\\r\\nEjercicio E0.1. Evaluaci\\xf3n inicial\\r\\nA0.1\\r\\nA0.1. Initial Assessment\\r\\nEjercicio E0.1. Evaluaci\\xf3n inicial Assignment A0.1 Intitial Assessment\\r\\nEjercicio E1.1. Encontrar a tu Buyer Persona A1.1\\r\\nA1.1. Finding your Buyer Persona\\r\\nAssignment A1.1 Finding your Buyer Persona\\r\\nEjercicio E1.2. Dise\\xf1ar el Viaje del Cliente A1.2\\r\\nA1.2. Customer Journey Mapping\\r\\nAssignment A1.2 Customer Journey Mapping\\r\\nEjercicio E2.1. SEO A2.1\\r\\nA2.1. SEO\\r\\nAssignment A2.1. SEO\\r\\nEjercicio E3.1. SEM A3.1\\r\\nA3.1. SEM\\r\\nAssignment A3.1. SEM\\r\\nEjercicio E4.1. Redes Sociales A4.1\\r\\nA4.1. Social Media\\r\\nAssignment A4.1. Social Media\\r\\nEjercicio E5.1. Anal\\xedtica A5.1\\r\\nA5.1. Analytics\\r\\nAssignment A5.1. Analytics\\r\\nTest T1.1 Q1.1\\r\\nQ1.1.\\r\\nQuiz Q1.1\\r\\nTest T2.1 Q2.1 Quiz Q2.1\\r\\nTest T3.1 Q3.1\\r\\nQ3.1.\\r\\nQuiz Q3.1.\\r\\nTest T4.1 Q4.1\\r\\nQ4.1.\\r\\nQuiz Q4.1.\\r\\nEncuesta \\x97 Survey\\r\\nTable 4.4: [Assignments] Assignments names transformation. The Spanish (left column)\\r\\nand the English (right columns) names in their different forms have been brought down\\r\\nto a common comparable name (central column). The survey has been discarded because\\r\\nnot relevant for further analysis.\\r\\ngroup_position 1 2\\r\\nposition 1 2 3 4 1 2 3 4 5\\r\\ncourse_id\\r\\n15 A0.1 A1.1 Q1.1 A1.2 \\x97 A2.1 \\x97 \\x97 Q2.1\\r\\n20 A0.1 \\x97 \\x97 \\x97 \\x97 A1.1 A1.2 \\x97 \\x97\\r\\n31 \\x97 A0.1 \\x97 \\x97 \\x97 A1.2 Q1.1 \\x97\\r\\n35 A0.1 A1.1 Q1.1 A1.2 \\x97 A2.1 \\x97 \\x97 Q2.1\\r\\n43 \\x97 \\x97 \\x97 \\x97 \\x97 A0.1 \\x97 \\x97 \\x97\\r\\n47 \\x97 \\x97 \\x97 \\x97 A0.1 A1.1 Q1.1 A1.2 \\x97\\r\\nTable 4.5: [Assignments] Quality issues of attributes group_position and position. It is\\r\\npossible to see, as an example, that the initial assignment A0.1 (highlighted in bold text)\\r\\nhas almost always different positions and group positions across different courses.\\r\\nFinally, the last attribute to investigate is points_possible, which tells which is the maximum\\r\\nscore for each assignment. However, since it is good to evaluate this attribute\\r\\ntogether with the points scored by the students, this field is analysed later while discussing\\r\\nabout the assignments submissions dataset.\\r\\n32\\r\\ngroup_name Module 0 Module 1 Module 2 Module 3 Module 4\\r\\nassignments weight assignments weight assignments weight assignments weight assignments weight\\r\\ncourse_id\\r\\n15 \\x97 \\x97 A0.1, A1.1, A1.2, Q1.1 25.0 A2.1, Q2.1 25.0 A3.1, Q3.1 25.0 A4.1, A5.1, Q4.1 25.0\\r\\n20 A0.1 0.0 A1.1, A1.2 25.0 A2.1, Q1.1 25.0 A3.1, Q3.1 25.0 A4.1, A5.1, Q4.1 25.0\\r\\n31 A0.1 0.0 A1.2, Q1.1 30.0 A2.1, Q2.1 20.0 A3.1, Q3.1 20.0 A4.1, A5.1, Q4.1 30.0\\r\\n35 \\x97 \\x97 A0.1, A1.1, A1.2, Q1.1 25.0 A2.1, Q2.1 25.0 A3.1, Q3.1 25.0 A4.1, A5.1, Q4.1 25.0\\r\\n43 A0.1 0.0 A1.1, A1.2, Q1.1 30.0 A2.1, Q2.1 20.0 A3.1, Q3.1 20.0 A4.1, A5.1, Q4.1 30.0\\r\\n47 \\x97 \\x97 A0.1, A1.1, A1.2, Q1.1 25.0 A2.1, Q2.1 25.0 A3.1, Q3.1 25.0 A4.1, A5.1, Q4.1 25.0\\r\\nTable 4.6: [Assignments] Assignment groups structure across different courses,\\r\\nhighlighting defects and inconsistencies in the data. The assignments that do not\\r\\naccounted for final grades are highlighted with strike-through text. It is possible to see\\r\\nhow the initial assignment belongs to different groups in different courses, but also how\\r\\nsame groups have different weights in different courses (e.g. group Module 4 in courses\\r\\n35 and 43) and how same assignments do not always count for the final grade (e.g. A2.1\\r\\nin course 15 and 20)\\r\\n\\x95 Items\\r\\nThis dataset contains all the details of each item accessible in the course and of their\\r\\ncorresponding modules. Item here stands for each element, content or resource accessible\\r\\nby the students through the course page (Figure 4.2). It must be noted that also\\r\\nassignments, quizzes and discussions created by teacher and staff are items because\\r\\nthey are always accessible from the contents page of the courses. However, since all\\r\\nthese three categories have their own datasets, they have been filtered out so to keep\\r\\nonly actual learning resources such as ExternalTools (video lectures), ExternalUrls and\\r\\nPages (links to external and internal resources) and Files.\\r\\nFigure 4.2: [Items] Count plot by type. ExternalTool represent the vast majority of items\\r\\navailable. Also, it is possible to notice that discussions, assignments and quizzes are all\\r\\nconsidered as items, but they have been filtered out.\\r\\nAlso in this case some irrelevant fields have been filtered out beforehand. Table 4.9\\r\\nshows the list of fields kept together with their missing values ratio. The first element\\r\\n33\\r\\nthat surprises is the high number of missing values for the publication dates of the\\r\\ncontents (unlock_at), meaning that it is not possible to know when each content was\\r\\npublished and for example to measure the time difference between content publication\\r\\nand consumption. Looking more in detail, the field appears to be missing for all items\\r\\nthat are not discussions or exams (Table 4.9b). To solve this issue, the proposed solution\\r\\nis to assume (without deviating too much from the reality of things) that the contents\\r\\nof a module are all unlocked when the module is unlocked, so the field is replaced with\\r\\nthe other field module_unlock_at.\\r\\nStill, this solution introduces the need to evaluate the quality of the data relative to\\r\\nthe modules. The modules are identified by an identifier (unique, not valid for comparison),\\r\\na name and a position. As for the assignments, these last two fields should\\r\\nbe comparable across different courses and Table 4.7 proves how this piece of data is\\r\\nactually consistent in this case; because of this, to facilitate the integration with the\\r\\nprevious dataset, the module_position has been simply replaced with the relative week\\r\\nof course (week = module_position ?? 1). The modules have also a publication date\\r\\n(module_unlock_at), but it has missing and incorrect information. Yet, comparing the\\r\\ncorrect dates with the dates in the assignments dataset (Table 4.3), it is possible to see\\r\\nthat most of the dates actually match, meaning that the assignments publication dates\\r\\noften match and could be used as the modules publication dates.\\r\\nweek 0 1 2 3 4-5\\r\\nmodule_position 1 2 3 4 5 6 7\\r\\ncourse_id\\r\\n15 M\\xf3dulo 0: ... M\\xf3dulo I: ... M\\xf3dulo II:... M\\xf3dulo III... M\\xf3dulo IV:... \\x97 Live Sessi...\\r\\n20 Module 0: ... Module I: ... Module II:... Module III... Module IV:... \\x97 \\x97\\r\\n31 Module 0: ... Module I: ... Module II:... Module III... Module IV:... Live Sessi... \\x97\\r\\n35 M\\xf3dulo 0: ... M\\xf3dulo I: ... M\\xf3dulo II:... M\\xf3dulo III... M\\xf3dulo IV:... Sesiones e... \\x97\\r\\n43 Module 0: ... Module I: ... Module II:... Module III... Module IV:... Live Sessi... \\x97\\r\\n47 M\\xf3dulo 0: ... M\\xf3dulo I: ... M\\xf3dulo II:... M\\xf3dulo III... M\\xf3dulo IV:... Sesiones e... \\x97\\r\\nTable 4.7: [Items] Comparison of modules names and position across different courses.\\r\\nSince the module names and their positions are coherent across all the courses, it was\\r\\npossible to simplify the modules to simple weeks (added on top). Also, additional\\r\\nmodules have been dropped because not relevant for the project.\\r\\nmodule_unlock_at\\r\\nweek 0 1 2 3 4 -5\\r\\ncourse_id\\r\\n15 2017-10-18 2017-10-20 2017-10-27 2017-11-03 2017-11-10\\r\\n20 2017-09-06 2017-09-08 2017-09-15 2017-09-22 2017-09-29\\r\\n31 2017-12-06 2017-09-06 2018-01-26 2018-02-02 2018-02-09\\r\\n35 2018-02-14 2018-02-16 2018-02-23 2018-03-02 2018-03-09\\r\\n43 NaN 2018-04-06 2018-01-26 NaN NaN\\r\\n47 2018-05-16 2018-05-18 2018-05-25 2018-06-01 2018-06-08\\r\\n(a)\\r\\nmodule_unlock_at\\r\\nweek 0 1 2 3 4 -5\\r\\ncourse_id\\r\\n15 2017-10-16 2017-10-18 2017-10-25 2017-11-01 2017-11-08\\r\\n20 2017-09-03 2017-09-08 2017-09-18 2017-09-25 2017-10-02\\r\\n31 2018-01-15 2018-01-19 2018-01-26 2018-02-02 2018-02-09\\r\\n35 2018-02-14 2018-02-16 2018-02-23 2018-03-02 2018-03-09\\r\\n43 2018-04-09 2018-04-13 2018-04-20 2018-04-27 2018-05-04\\r\\n47 2018-05-16 2018-05-18 2018-05-25 2018-06-01 2018-06-08\\r\\n(b)\\r\\nTable 4.8: [Items] Publication dates of weekly modules, before (a) and after (b) data\\r\\ncleaning. Given the inconsistency of data, all the dates have been substituted using the\\r\\ndates from the assignments dataset.\\r\\n34\\r\\nThe second field with most missing values is content_id. Also in this case, the field\\r\\nhappens to be missing for two specific types of items: Page and ExternalTool.\\r\\nLastly, the URLs: quite surprisingly by looking at Table 4.9a it is possible to see that\\r\\nthe field url is missing only in the case of items whose type is ExternalUrl and that on\\r\\nthe other hand the field external_url is present only in the case of ExternalUrls. The\\r\\neasiest solution was to fill missing url values with external_tool values and drop this\\r\\nlast column.\\r\\n\\x95 Quizzes Questions\\r\\nThis dataset contains very basic statistics about each question in courses quizzes. Like\\r\\nin many other cases, since Canvas allows to access only data of one course and quiz at\\r\\na time, the dataset is obtained by iterating the call for (course, quiz) pair. Table 4.10\\r\\nshows a description of the fields available. Since it has no missing values and data are\\r\\nconsistent, no further manipulation is required.\\r\\nStudents\\x92 Activity\\r\\n\\x95 Authentication Events\\r\\nThis dataset contains logins and logouts for all course students. Actually, Canvas allows\\r\\nonly to access the authentication events by single user5, so this dataset is built by\\r\\nappending to each other all the results obtained by repeating the call for all the students.\\r\\nThe dataset has very limited information: the id of the event, the time-stamp\\r\\nand the type of event (login or logout). The dataset has no null values, but the most\\r\\ninteresting thing to notice here is that the number of logins and logouts is extremely\\r\\nunbalanced (Figure 4.3). This is because although users must perform a login event to\\r\\naccess the course materials, they are not forced to logout so they rather let the session\\r\\nexpire. This is a quite important issue because it makes impossible to have an accurate\\r\\nidea of users\\x92 sessions duration. Also, since the dataset has no information that make\\r\\npossible to integrate it, for example, with page views, it is not of much use and it has\\r\\nbeen discarded.\\r\\n\\x95 Page Views\\r\\nAs in the case of authentication events, also in this case Canvas API allows only to\\r\\naccess page views per single user so the dataset has been built by repeating the call\\r\\nper each student and putting results together. This is surely one of the most important\\r\\ndatasets to extract information about students\\x92 studying habits.\\r\\nTable 4.11 shows all the fields of the dataset. The first attribute worth inspecting is\\r\\nparticipated and its relation with the field http_method (Figure 4.4b): it emerges that\\r\\nparticipated is False if and only if the HTTP request was GET and, on the other hand, it\\r\\nis True only if the request is POST. Because of this, the field participated was dropped.\\r\\nAlso, given that POST requests cannot be really interpreted as page views (they are\\r\\nrather actions) they have been removed. Eventually, only GET requests have been kept\\r\\nand also the field http_method was removed.\\r\\n5https://canvas.instructure.com/doc/api/authentications_log.html#method.authentication_audit_api.for_user\\r\\n35\\r\\nName Type Description Nulls (%)\\r\\nid int unique id for the item \\x97\\r\\ncontent_id int 26.02%\\r\\ncourse_id int id of the course the item belongs to \\x97\\r\\nexternal_url string URL of the external resource the\\r\\nitem refers to\\r\\n21.16%\\r\\nitem_url string Canvas URL to access the item page \\x97\\r\\nlocked_for_user bool whether or not the item is accessible\\r\\nto users\\r\\n\\x97\\r\\nmodule_id int id of the module the item belongs to \\x97\\r\\nmodule_name str name of the module the item belongs\\r\\nto\\r\\n\\x97\\r\\nmodule_position int position of the module the items belongs\\r\\nto\\r\\n\\x97\\r\\nmodule_unlock_at datetime when the module of the item was\\r\\npublished\\r\\n10.12%\\r\\nposition int position of the item in the course\\r\\nstructure\\r\\n\\x97\\r\\npublished bool whether or not the item has been\\r\\npublished\\r\\n\\x97\\r\\nquiz_module_position int ... \\x97\\r\\ntitle string title of the item visible to users \\x97\\r\\ntype string type of item; possible values: ExternalTool,\\r\\nExternalUrl, Page, Discussion,\\r\\nFile, Assignment, Quiz.\\r\\n\\x97\\r\\nunlock_at datetime when the item was published 89.62%\\r\\nurl string main URL of the item 21.16%\\r\\n(a)\\r\\nMissing publication dates\\r\\nfield unlock_at\\r\\nis_missing True False\\r\\ntype\\r\\nDiscussion \\x97 18.0\\r\\nQuiz \\x97 21.0\\r\\nAssignment 4.0 40.0\\r\\nPage 37.0 \\x97\\r\\nFile 41.0 \\x97\\r\\nExternalUrl 161.0 \\x97\\r\\nExternalTool 439.0 \\x97\\r\\n(b)\\r\\nMissing URLs\\r\\nfield url external_url\\r\\nis_missing False True False True\\r\\ntype\\r\\nAssignment 44.0 \\x97 \\x97 44.0\\r\\nDiscussion 18.0 \\x97 \\x97 18.0\\r\\nExternalTool 439.0 \\x97 439.0 \\x97\\r\\nExternalUrl \\x97 161.0 161.0 \\x97\\r\\nFile 41.0 \\x97 \\x97 41.0\\r\\nPage 37.0 \\x97 \\x97 37.0\\r\\nQuiz 21.0 \\x97 \\x97 21.0\\r\\n(c)\\r\\nTable 4.9: [Items] Fields description (a) and missing values inspection (b and c).\\r\\n36\\r\\nField Type Description\\r\\nid int Unique identifier for the question\\r\\nquiz_id int id of the quiz the question belongs to\\r\\nquestion int Number of the question within the given\\r\\nquiz\\r\\ncorrect_student_count int Number of students who answered the\\r\\nquestion correctly\\r\\nincorrect_student_count int Number of students who did not answer the\\r\\nquestion correctly\\r\\nquiz_statistics_points_possible int Number of maximum points possible for\\r\\nthe question\\r\\ncorrect_student_ratio float ratio of students who answered correctly\\r\\nover students who answered incorrectly\\r\\nincorrect_student_ratio float ratio of students who answered incorrectly\\r\\nover students who answered correctly\\r\\nquiz_name string Comparable name of the quiz (as described\\r\\nin Table 4.4)\\r\\ncourse_id id id of the course the quiz and the question\\r\\nbelong to\\r\\nTable 4.10: [Quiz Questions] Fields description.\\r\\nFigure 4.3: [Authentication events] Events count by type across all students data. It\\r\\nemerges how logins and logouts are imbalanced in favor of logins. This is because users\\r\\nrarely close their sessions and rather wait for the session to expire.\\r\\nMoreover, looking at the field context_type it is possible to see that the page refer to\\r\\ndifferent contents such as Course, User, UserProfile, Account or do not have any content\\r\\n(see Figure 4.4c). By inspecting more in detail this field it was decided that only\\r\\nthe page views that refer to Course content are actually worth considering so all the\\r\\nrest have been discarded. On top of that, once only Course context had been selected,\\r\\nthe field context_id happened to be the identity of the course each page view referred\\r\\nto. This meant that, theoretically, the context_id of each page should correspond to the\\r\\n37\\r\\nField Type Description Nulls (%)\\r\\nid int unique id for the page view \\x97\\r\\nsession_id int id for the browser session; it changes every\\r\\ntime a new window is opened\\r\\n\\x97\\r\\ncontext_type string type of context from which the request was\\r\\nmade; in the practice it always corresponds to\\r\\nthe string \"course\"\\r\\n12%\\r\\ncontext_id int id of the context from which the request was\\r\\nmade; in the practice it is the id of the course\\r\\nthe page view refers to\\r\\n13%\\r\\ncontroller string context section the request refers \\x97\\r\\naction string type of action requested \\x97\\r\\ncreated_at datetime date and time the http request was created at \\x97\\r\\nurl string the url requested \\x97\\r\\nhttp_method string the http method of the request (i.e. GET,\\r\\nPOST, PUT)\\r\\n\\x97\\r\\nremote_ip string the ip from which the request was sent \\x97\\r\\nuser_id int id of the user who requested the url \\x97\\r\\nparticipated bool whether the http request counted as participating,\\r\\nsuch as submitting homework\\r\\n\\x97\\r\\ncourse_id int id of the course attended by the student who\\r\\nwatched the page\\r\\n\\x97\\r\\nTable 4.11: [Page views] Fields description.\\r\\ncourse_id (i.e., the id of the course) attended by the student who visited the page (since\\r\\nstudents clearly cannot access contents from other courses they are not enrolled in!).\\r\\nInterestingly enough, a quick analysis found out the presence of entries in which these\\r\\ntwo fields did not match (Figure 4.4d): this is due to the fact that some students are also\\r\\nmembers of the institution and had sporadic access to contents/resources belonging to\\r\\ncourses they did not attend. Clearly, these views out of the correct context have been\\r\\ndropped, too.\\r\\nAt this point, an important thing to consider is that this dataset is the only one from\\r\\nwhich it is possible to determine which pages and which content each student visited.\\r\\nHowever, the dataset as it is does not provide an explicit field that allows to merge it with\\r\\nthe dataset containing all the course items and their details. Looking at the attributes of\\r\\nthe dataset it is possible to conclude that the only way to match the page views dataset\\r\\nwith the items dataset is to use the URLs. In the following section it will be explained\\r\\nwhy and how these two datasets have been merged.\\r\\n\\x95 Discussion Posts\\r\\nThis dataset contains all the messages posted in the discussion forums. Actually, Canvas\\r\\ndoes not provide a direct access to this kind of dataset and some workarounds are neces-\\r\\n38\\r\\n(a)\\r\\n(b)\\r\\n(c)\\r\\n(d)\\r\\nFigure 4.4: [Page views] Data exploration and filtering steps. Firstly, only GET requests\\r\\nare selected (Figure 4.4b), then an inspection of context_type and controller allowed to\\r\\nfilter out all page views that do not refer to \"Course\" context (4.4c and Figure 4.4a).\\r\\nFinally, the page views relative to other courses than the course attended are also filtered\\r\\nout (Figure 4.4d).\\r\\n39\\r\\nsary, iterating first over the topics of one course (Table 4.12a), then for each topic, over\\r\\nthe main replies and finally, for each main reply, over all the nested replies recursively\\r\\n(Table 4.12b).\\r\\ntitle\\r\\nmessage\\r\\ncreated_at\\r\\nallow_rating\\r\\ncomments_disabled\\r\\nid\\r\\n138 E1.2. Dise\\xf1ar... <p>En este hilo... 2017-10-23\\r\\n17:54:13\\r\\nTrue False\\r\\n140 Visibilidad y ... <p>\\xbfTienes una... 2017-10-15\\r\\n22:25:21\\r\\nTrue False\\r\\n142 Pres\\xe9ntate... <p>Pres\\xe9ntate\\r\\nante...\\r\\n2017-10-15\\r\\n21:59:19\\r\\nTrue False\\r\\n(a)\\r\\nuser_id\\r\\nmessage\\r\\nparent_id\\r\\nrating_count\\r\\nreplies\\r\\ncreated_at\\r\\nid\\r\\n1733 32 <p>Hi everyone,</\\r\\np>\\\\n<p>I\\r\\nhope...\\r\\nNaN NaN NaN 2018-02-14\\r\\n14:48:54\\r\\n1744 403 <div></div>Hi\\r\\nEveryone!!<\\r\\ndiv><br>...\\r\\nNaN 3.0 [{\\x92id\\x92: 1748,\\r\\n\\x92user_id\\x92: 32,\\r\\n\\x92message\\x92: ...\\r\\n2018-02-14\\r\\n21:19:55\\r\\n1818 393 <p>At my side on\\r\\nsocial post...\\r\\nNaN NaN [{\\x92id\\x92: 1826,\\r\\n\\x92user_id\\x92: 32,\\r\\n\\x92message\\x92: ...\\r\\n2018-02-20\\r\\n07:59:41\\r\\n(b)\\r\\nTable 4.12: [Discussion Forums] Example datasets: topics (i.e. main posts) (a) and main\\r\\nposts (b). Students\\x92 replies are nested in the replies object of the latter (highlighted in\\r\\nbold text).\\r\\nCertainly the core of this dataset are the messages, on which is possible to perform a lot\\r\\nof text mining. As it is possible to see in Table 4.13, the additional features extracted\\r\\nare several and will be discussed more thoroughly in the following sections.\\r\\n40\\r\\nName Type Description Nulls (%)\\r\\nid int unique id for the message posted \\x97\\r\\ncourse_id int id of the course the discussion belongs to \\x97\\r\\ntopic_id int id of the topic the message belongs to \\x97\\r\\nparent_id int id of the parent message (the message the\\r\\ncurrent message is reply of)\\r\\n3%\\r\\ncreated_at datetime when the message was posted \\x97\\r\\nuser_id int id of the author of the message \\x97\\r\\nmessage string text of the message \\x97\\r\\nrating_count float number of likes received 7%\\r\\nn_replies float number of replies to the message \\x97\\r\\nparent_user_id float author of the parent message 3%\\r\\nlength int number of characters of the message \\x97\\r\\ntokens list list of tokens of the message \\x97\\r\\nwords list list of words of the message \\x97\\r\\nn_sentences int number of sentences of the message \\x97\\r\\nn_questions int number of questions in the message \\x97\\r\\nn_punct int number of punctuation characters \\x97\\r\\nn_tokens int number of tokens \\x97\\r\\nn_words int number of words \\x97\\r\\nn_unique_words int number of unique words \\x97\\r\\nn_paragraphs int number of paragraphs \\x97\\r\\nn_newlines int number of carriage returns \\x97\\r\\nn_emoticons int number of emoticons in the message \\x97\\r\\nTable 4.13: [Discussion Messages] Fields description.\\r\\nFour important things have to be highlighted:\\r\\n1. while the entries and the replies have the field rating_count the topics do not. This\\r\\nis because it is only possible to like only entries (and replies), not threads;\\r\\n2. the rating fields is empty not only in the case of entries with no likes but also in\\r\\nthe case of posts that cannot be liked. To properly handle this field, it has been\\r\\nfilled with 0 in all those topics that had allow_rating equal to True, while it has\\r\\nbeen left null for the other topics;\\r\\n3. the entries dataset (and all the nested replies) does not have an explicit field for\\r\\nthe number of replies to each message. This field had to been added (by simply\\r\\nlooking at the length of the list object in replies);\\r\\n4. the new dataset just obtained, although fundamental, provides only limited information.\\r\\nWhile it is true that it is possible to count the messages posted by user\\r\\nor the number of likes received by user, it is also true that it does not provide any\\r\\ninformation about the received likes or whether or not each post has been read by\\r\\n41\\r\\nFigure 4.5: Assignments grades comparison for consecutive intakes\\r\\na given user, which are elements very useful to measure students\\x92 activity. This\\r\\nmade necessary to build additional datasets.\\r\\n\\x95 Submissions\\r\\nThis dataset contains one entry per assignment and quiz submission by each student.\\r\\nThe complete list of fields can be found in Table 4.14.\\r\\nOne thing to note here is that, by inspecting the attribute points_possible for each assignment\\r\\nin a given course and comparing it with the average grades (Table 4.15), it\\r\\nis possible to find out that the values are not consistent (like quizzes in course 47 are\\r\\ngraded over 100 while over 10 in all the others) and in one case not even coherent (A0.1\\r\\nand A1.1 for course 15). While in the former case the score can be normalized by simply\\r\\ntaking the ratio of points scored over points possible, in the latter data had to be\\r\\nmanually changed.\\r\\nOnce cleaned, this dataset is extremely useful to get insights about grades and intakes\\r\\nin general, which is one of the business goals set in Chapter 1. As an example Figure\\r\\n4.5 shows a box plot with grades distribution comparison for all the assignments\\r\\nacross different intakes. In this plot it is possible to see, for example, that the first two\\r\\nassignments (A1.1 and A1.2) are constantly year after year the ones with highest scores,\\r\\nwhile the admission test (A0.1) has always the lowest ones; also, it is quite evident that\\r\\nthe fourth assignment (A4.1) is gradually reporting lower and lower scores, with almost\\r\\nall the students in the last intake (intake 47, brown-coloured) failing the assignment,\\r\\nthus suggesting that either the explanation of the topics or the quality of the learning\\r\\nresources should be improved.\\r\\n\\x95 Peer Reviews\\r\\nThe term peer review indicates the evaluation of a student\\x92s assignment by one or multiple\\r\\nstudents, i.e. peers. This dataset contains all the information regarding who assessed\\r\\n42\\r\\nName Type Description Nulls (%)\\r\\nstudent_id int ID of the user who did the submission\\r\\n\\x97\\r\\nassignment_id int ID of the assignment submitted \\x97\\r\\ndue_at datetime due date for the submission of\\r\\nthe assignment\\r\\n4.15%\\r\\nexcused bool whether the student was excused\\r\\nfor the corresponding assignment\\r\\n\\x97\\r\\nassignment_name string name of the assignment submitted\\r\\n\\x97\\r\\npoints_possible int maximum number of points it is\\r\\npossible to score in the corresponding\\r\\nassignment\\r\\n\\x97\\r\\nstatus string status of the submission \\x97\\r\\nsubmission_score float grade for the submission 18.34%\\r\\nsubmission_submitted_at datetime timestamp of the submission 17.83%\\r\\nunlock_at datetime timestamp when the corresponding\\r\\nassignment was firstly published\\r\\n5.90%\\r\\nis_quiz_assignment bool whether the corresponding assignment\\r\\nis a quiz or not\\r\\n\\x97\\r\\ngroup_weight float weight of the group which the\\r\\nsubmitted assignment belong\\r\\n\\x97\\r\\ncomparable_name string standardized name of the submitted\\r\\nassignment\\r\\n5.28%\\r\\nweek int week of course the submitted assignment\\r\\naccount for (N.B. it\\r\\ncorresponds to the first number\\r\\nin the comparable_name)\\r\\n5.28%\\r\\nTable 4.14: [Submissions] Fields description.\\r\\nname A0.1 A1.1 A1.2 Q1.1 A2.1 Q2.1 A3.1 Q3.1 A4.1 Q4.1 A5.1\\r\\ncourse_id\\r\\n15 14.44 / 0* 98.30 / 0* 95.90 / 100 6.62 / 10 79.75 / 100 8.67 / 10 79.65 / 100 8.56 / 10 84.44 / 100 15.33 / 20 80.50 / 100\\r\\n20 17.27 / 100 85.07 / 100 91.75 / 100 7.36 / 10 83.33 / 100 X 81.25 / 100 7.50 / 10 70.83 / 100 14.18 / 20 44.17 / 100\\r\\n31 18.89 / 100 X 99.00 / 100 6.67 / 10 86.11 / 100 7.78 / 10 86.11 / 100 7.71 / 10 70.83 / 100 14.00 / 20 80.00 / 100\\r\\n35 25.40 / 100 96.70 / 100 99.70 / 100 8.50 / 10 87.50 / 100 8.90 / 10 80.00 / 100 9.20 / 10 82.50 / 100 15.30 / 20 96.60 / 100\\r\\n43 8.93 / 100 99.43 / 100 98.29 / 100 7.64 / 10 73.12 / 100 7.50 / 10 68.75 / 100 8.23 / 10 64.42 / 100 15.62 / 20 98.58 / 100\\r\\n47 9.22 / 100.0 88.89 / 100 88.89 / 100 74.44 / 100 69.44 / 100 75.56 / 100 44.44 / 100 84.44 / 100 31.94 / 100 77.78 / 100 92.00 / 100\\r\\nTable 4.15: [Students\\x92 Grades] Average score per assignment compared to points\\r\\npossible. Incoherent and inconsistent data have been highlighted in bold text. Values\\r\\nmarked with a star (*) have been replaced with 100.\\r\\n43\\r\\nwhom across all assignments: the main information is assessing and assessed students\\x92\\r\\nids, status of the peer review and id of the assignment graded. Although the assessor\\r\\nmight actually be a group of people (who evaluate and grade together a given assignment),\\r\\nin this case more students may grade the same student bu all separately. An example\\r\\nis shown in Table 4.16: user 393 is assessed by user 391 and user 398 but while\\r\\nthe second user completed the review, the former did not (see field workflow_state).\\r\\nThis difference in the status of the review would be impossible if the two users were\\r\\ngrading as a group.\\r\\nassessor_user_id assessed_user_id workflow_state assignment_id course_id\\r\\nid\\r\\n628 390 402 completed 272 31\\r\\n627 390 403 completed 272 31\\r\\n622 391 393 assigned 272 31\\r\\n621 391 398 assigned 272 31\\r\\n634 392 394 completed 272 31\\r\\n633 392 395 completed 272 31\\r\\n625 393 391 completed 272 31\\r\\n626 393 398 completed 272 31\\r\\n637 394 392 completed 272 31\\r\\n638 394 395 completed 272 31\\r\\n635 395 392 completed 272 31\\r\\n636 395 394 completed 272 31\\r\\n623 398 391 completed 272 31\\r\\n624 398 393 completed 272 31\\r\\n631 402 390 completed 272 31\\r\\n632 402 403 completed 272 31\\r\\n629 403 390 completed 272 31\\r\\n630 403 402 completed 272 31\\r\\nTable 4.16: [Peer Reviews] Example dataset\\r\\n\\x95 Peer Reviews Comments\\r\\nIn addition to the submission of the review, users (i.e., staff, teachers and students) can\\r\\nalso add a comment to it to elaborate a bit more or add some notes about the review or\\r\\nthe assignment. Since according to teachers and staff adding comments to peer reviews\\r\\nshould be taken into account, also these comments have been retrieved and structured\\r\\nin a dataset as shown in Table 4.17.\\r\\n4.1.3 Data Augmentation\\r\\nUnderstanding data means identifying its shortcomings and overcome them, too. Analysing\\r\\nthe information in the datasets presented in the previous section and crossing available\\r\\ndata with the business goals defined in Section 1.2, it was possible to conclude that three\\r\\n44\\r\\nField Type Description\\r\\nassessing_user_id int ID of the assessing user in the commented peer review\\r\\nassessed_user_id int ID of the assessed user in the commented peer review\\r\\nassignment_id int ID of the assignment reviewed\\r\\nauthor_id int ID of the user who posted the comment\\r\\ncourse_id int ID of the course the assignment refers to\\r\\ncreated_at datetime Time-stamp when the comment was published\\r\\nid int Unique id of the comment posted\\r\\nmessage string Text of the comment\\r\\nTable 4.17: [Peer Reviews Comments] Fields description\\r\\nmany pieces of information were still missing. The new datasets that were needed are\\r\\nmainly three:\\r\\n1. item views: item view here is intended as a page view that refers to an item, that\\r\\nis the view of a content, a learning resource. Since the page views dataset does not\\r\\nprovide any information about the items visited, this new dataset is the result of the\\r\\nmerge of page views and items datasets;\\r\\n2. ratings: the discussion datasets provides only information about the number of\\r\\nlikes received by each post. However, it is possible to trace back the author of each\\r\\nlike and build a new dataset containing one row per message and one column per\\r\\nauthor, and either 0 or 1 in each cell whether the post on the corresponding row was\\r\\nrated or not by the author on the corresponding column;\\r\\n3. read posts: since reading posts has been considered as an important indicator of\\r\\nstudent activity and engagement, this is a very interesting piece of information that\\r\\nwas not made available in any of the datasets presented. However, it has been\\r\\npossible to find a word-around for that so to create a new dataset containing for\\r\\neach discussion topic and user the number of unread posts. Taking the difference\\r\\nbetween available posts and unread posts per each topic, it is possible to trace back\\r\\nthe number of read posts per user.\\r\\nHere below more details are provided for each of these two datasets.\\r\\n\\x95 Item Views\\r\\nThis dataset is necessary to identify which page views actually refer to the visualization\\r\\nof a learning resource (video lecture, link or file) and, for example, count the number\\r\\nof contents viewed by each student. The goal is to associate to each page view the id of\\r\\nthe corresponding item (if any).\\r\\nThe only piece of information on which it is possible to merge the page views and the\\r\\nitems dataset just described is the URL of the page views. Reviewing the attributes of\\r\\nthese two datasets (see Table 4.9a and Table 4.11 for items and page views, respectively),\\r\\nit is possible to see that the page views dataset has only one URL, while the\\r\\n45\\r\\nitems have three fields: url, item_url and external_url. As it is possible to see in Table\\r\\n4.18, the URLs are far from being standardized and there are a lot of differences:\\r\\nfor example, some URLs end with a backslash while others do not, or some have additional\\r\\nparameters while some others have not. Clearly, joining on such long and\\r\\nnon-standardized URLs would result in very few matches. On the other hand, there is\\r\\nno way to get every match between the item urls and the visited ones. To still get an\\r\\nacceptable merge the following protocol was adopted:\\r\\ntype field value\\r\\nitem_id\\r\\n3481 Page url https://impactonline.ie.edu/courses/31/pages/googlemicro-\\r\\nmoments-christian\\r\\nitem_url https://impactonline.ie.edu/courses/31/modules/items/3481\\r\\nexternal_url NaN\\r\\n3485 File url https://impactonline.ie.edu/courses/31/files/3080\\r\\nitem_url https://impactonline.ie.edu/courses/31/modules/items/3485\\r\\nexternal_url NaN\\r\\n3483 ExternalUrl url https://www.thinkwithgoogle.com/nordics/article/understandingthe-\\r\\ncustomer-journey-three-critical-insights/\\r\\nitem_url https://impactonline.ie.edu/courses/31/modules/items/3483\\r\\nexternal_url https://www.thinkwithgoogle.com/nordics/article/understandingthe-\\r\\ncustomer-journey-three-critical-insights/\\r\\n3488 ExternalTool url https://impactonline.ie.edu/courses/31/external_tools/sessionless_item_url https://impactonline.ie.edu/courses/31/modules/items/3488\\r\\nexternal_url https://iexl.instructuremedia.com/lti/launch?custom_arc_launch_type=818a-4ff6-b900-8bbfe417d836-12\\r\\nTable 4.18: [Items] Comparison for three URL fields (url, item_url and external_url)\\r\\nacross different item types.\\r\\n\\x96 from Table 4.18 it is possible to see that in some cases the ID of the corresponding\\r\\nitem is already specified in the URL. These cases are two: it comes in the form\\r\\n\"http://..../items/(ID)\" or as a parameter in the form \"http://....?...&module_item_id=(ID).\\r\\nBecause of this, regular expressions were run over all the page views URLs to extract\\r\\nsuch IDs;\\r\\n\\x96 after this, all URLs have been standardized: this meant removing http protocol\\r\\nkeywords and all http parameters (anything that comes after the question mark),\\r\\nremoving possible double slashes and remove the possible final backslash. Once\\r\\nall the URLs were standardized (both in the page views as in the items datasets),\\r\\nthe page views dataset was joined three times with the items dataset using each\\r\\ntime a different URL field as key.\\r\\n46\\r\\n32 96 353 474 475 476 478 479 480 user_id\\r\\npost_id\\r\\n3144 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 32.0\\r\\n3165 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 479.0\\r\\n3221 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 32.0\\r\\n3284 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 96.0\\r\\n3293 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 475.0\\r\\n...\\r\\n3790 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 476.0\\r\\n3795 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 476.0\\r\\n3798 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 32.0\\r\\n3800 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 32.0\\r\\n3482 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\\r\\n(a)\\r\\nn_posts 96 353 473 474 475 476 478 479 480\\r\\nid\\r\\n732 50 20.0 24.0 50.0 9.0 0.0 8.0 8.0 10.0 16.0\\r\\n733 25 3.0 21.0 0.0 0.0 0.0 1.0 10.0 8.0 7.0\\r\\n734 16 4.0 1.0 15.0 0.0 0.0 0.0 12.0 5.0 5.0\\r\\n735 58 11.0 11.0 48.0 13.0 0.0 6.0 12.0 5.0 5.0\\r\\n736 7 4.0 6.0 3.0 0.0 0.0 2.0 0.0 2.0 5.0\\r\\n...\\r\\n740 37 4.0 20.0 12.0 13.0 1.0 2.0 1.0 8.0 17.0\\r\\n741 38 8.0 29.0 31.0 7.0 1.0 4.0 1.0 10.0 4.0\\r\\n742 65 10.0 8.0 47.0 14.0 0.0 3.0 1.0 11.0 1.0\\r\\n743 29 7.0 9.0 4.0 10.0 2.0 17.0 13.0 14.0 18.0\\r\\n906 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\\r\\n(b)\\r\\nTable 4.19: Additional datasets created for given likes (a) and unread posts (b).\\r\\nAs already mentioned earlier, this dataset is very useful to get insight about the course\\r\\nand the way it is \"consumed\".\\r\\n\\x95 Ratings\\r\\nUnfortunately, Canvas API does not have a specific call to directly retrieve information\\r\\nabout ratings. Yet, it has been possible to find a workaround that allows to get for each\\r\\ntuple (course; topic; user) a list of messages within the topic liked by the user. Thus,\\r\\niterating this call for all users and all topics allows to get a list of all posts liked by each\\r\\nuser. This results in a dataset very similar to the one shown in Table 4.19a, with posts\\r\\non the rows and users on the columns and each cell containing 1 or 0 whether or not\\r\\nthe post on the corresponding row was liked by the user on the corresponding column.\\r\\nAlso, merging the resulting dataset with the discussion messages dataset, it has been\\r\\npossible to add the information about the original author of each post and the date of\\r\\npublication. The resulting dataset allows to compute the total number of received and\\r\\ngiven likes by grouping by user on the rows and columns, respectively, and summing\\r\\nup all the values.\\r\\n\\x95 Read posts\\r\\nVery similarly to the case of ratings, it has been possible to find a workaround that\\r\\nallows to get for each tuple (course; topic; user) the number of unread posts. Iterating\\r\\nthe call for all users and topics allows to get a dataset like the one shown in Table 4.19b.\\r\\nThen, knowing from the topics dataset the total number of posts per topic, it is possible\\r\\nto compute for each row the difference between total number of posts and the number of\\r\\nunread posts to get the number of read posts. Summing across the rows gives the total\\r\\nnumber of unread posts for each student, but it is also possible to compute the average\\r\\nread post ratio.\\r\\nAlso, in the case of ratings and read posts it is worth to remember that it is not always\\r\\ngold all that glitter. Indeed, although the functions/calls here described allow to get\\r\\ninformation about the posts rated by a student or the number of posts read, they do not\\r\\nprovide any indication of time. That is, the information is relative to the time-stamp of\\r\\ndownloading. To put an example, given 10 posts and two students, if student A read them\\r\\none per day as soon as they were posted for 10 days while student B read them all on the\\r\\n47\\r\\n(a)\\r\\n(b)\\r\\nFigure 4.6: Example of insightful visualizations that it is possible to extract from item\\r\\nviews dataset. On the top, each subplot shows the trend of views of the contents of a\\r\\ndifferent week of course, highlighting whether students view contents within the same\\r\\nweek or later; on the bottom, items are ranked by number of views to identify which\\r\\ncontents have generated the most and the least interest among students.\\r\\n48\\r\\nvery last day, the result of the procedure described above could only tell that student A\\r\\nand B read the same amount of posts, but it would not give any idea of how they did it.\\r\\nClearly, the same applies for ratings. This is very important because it means that it is not\\r\\npossible to put likes and read posts on a timeline and compute them a posteriori over an\\r\\narbitrary interval of time. While using the publication date of messages it is possible to\\r\\ncount the number of posted messages per day or per week grouping by date, this is not\\r\\npossible for these two pieces of data. The only way possible would be to download the\\r\\ninformation each day or week during the course and then integrate it.\\r\\n4.2 Data Preparation\\r\\nIn this section the focus will move towards how the dataset just described have been used\\r\\nto produce the final features and the final datasets needed to do modelling, that is to predict\\r\\nstudents performance or identify the factors that maximize knowledge and engagement.\\r\\nThe general idea is to end up with a dataset containing one row per each student and\\r\\none column per each indicator/variable/feature selected. Also, since the variables change\\r\\nover time, it would be great to have a dataset per each day of course. This would allow\\r\\nnot only to show the evolution of variables over time to see if they are stationary, but\\r\\nalso to train different models for different time intervals so to pick up the one that gives\\r\\nacceptable results as soon as possible. Because of this, each indicator will be a function\\r\\nof two parameters: the user (more precisely: the student) and the time (as in number of\\r\\ndays from the start of the course).\\r\\nAlso, it must be noted that since different students attended different courses (more\\r\\nprecisely: different intakes), there is also a third hidden parameter: the intake. Rather than\\r\\nhidden, this dimension was actually overlooked: since the number of students per intake\\r\\nis very small and also the overall number of intakes is modest, the only way to build\\r\\nup a dataset with an almost acceptable number of rows was not to put much attention\\r\\nin the single intake and consider all students as belonging almost to the same course,\\r\\noverlooking the issue about whether data from different intakes are actually comparable\\r\\nor not. Still, to mitigate the differences about intakes, all the values have been somehow\\r\\nstandardized.\\r\\nGeneral Notation\\r\\nBefore proceeding with the description of the selected features, it seems wise to define\\r\\nsome general notation and rules:\\r\\n\\x95 the calligraphic font is used for sets, like sets of words but also sets of IDs. For\\r\\nexample:\\r\\n\\x96 U stands for the entire set of users;\\r\\n\\x96 C stands for the entire set of courses (best: intakes);\\r\\n\\x96 D stands for the set of days of course (numbers from 1 to 35, since the intakes\\r\\nlast 5 weeks);\\r\\n49\\r\\n\\x95 the blackboard font is used for whole datasets, for example:\\r\\n\\x96 M stands for the entire dataset of messages posted in forums;\\r\\n\\x96 J stands for the entire dataset of item views;\\r\\n\\x96 P stands for the entire dataset of page views;\\r\\nAlso:\\r\\n\\x95 when a dataset is subscripted, the subscript acts like a rows slicer and only the rows\\r\\nthat match the subscript are selected: for example subscripting a dataset with u it\\r\\nmeans that only the rows relative to the user u are selected. As a general rule, u\\r\\nis always used to indicate the user ID, while D is used to refer to dates (the name\\r\\nof the field may change in different datasets). As an example, Mu;d\\x14D is used to\\r\\nindicate the sub-dataset of messages written by user u no later than the Dth day of\\r\\ncourse; in Python Pandas it would be equivalent to:\\r\\n>>> df = df.loc[(df.user_id == u)\\r\\n& (df.created_at.apply(get_day_of_course) <= D), :]\\r\\n\\x95 when a dataset is superscripted, the superscript acts like a columns slicer and only\\r\\nthe columns that are specified in the superscript are selected: for example Purl\\r\\nindicates the list of URLs of all students\\x92 page views; in Python Pandas it would be\\r\\nequivalent to the following line of code:\\r\\n>>> df = df.loc[:, [\\x92url\\x92]]\\r\\n.\\r\\n4.2.1 Features Extraction\\r\\nAfter having reviewed the state of the art and having seen the most frequent indicators\\r\\nused in the literature, these variables have been eventually cross-matched with the pieces\\r\\nof data available to see which of them it was actually possible or it made sense to compute.\\r\\nFrom this analysis it emerged that not all of them were actually feasible or relevant for the\\r\\nspecific case of HIOPs so some have been removed. Still, it must be also noted that after\\r\\nhaving discussed with staff an managers some others have been added. As an example,\\r\\nthe number of clicks or the time on platform were two interesting variables used by many\\r\\nscholars but they were not made available by Canvas REST API. Also, the number of\\r\\ncomments edited or the number of assignments submitted were among the most popular\\r\\nindicators used in literature, but in this scenario the number of edits or the number of nonsubmitted\\r\\nassignments were so low that both variables were discarded. On the other hand,\\r\\nhowever, almost nobody performed some text mining and extracted new features from\\r\\nstudents\\x92 messages although very interesting according to this author, so they have been\\r\\nadded despite missing prior references in the literature. Also, in none of the cited works\\r\\nthere has been specific mention about peer reviews and how many comments student\\r\\nmake to peer reviews, but as they are one of the key point in HIOPs they have been taken\\r\\ninto account.\\r\\nRecall from Section 1.2.1 that the four major KPIs to be addressed in feature extraction\\r\\nare:\\r\\n50\\r\\n1. assignments & quizzes;\\r\\n2. discussion forums & communication;\\r\\n3. learning resources;\\r\\n4. time on platform.\\r\\nIn the following subsections all the features will be presented following this order,\\r\\neach associated to the KPI they are meant to measure:\\r\\n1. Assignments & Quizzes\\r\\n(a) grades_admission It is simply the grade in the admission test.\\r\\n(b) grades_assg It is the average grade in the assignments (excluding quizzes)\\r\\nsubmitted before the Dth day of course.\\r\\n(c) grades_quiz It is the average grade in the quizzes submitted before the Dth\\r\\nday of course.\\r\\n2. Communication\\r\\n(a) n_messages This variable takes into account the number of posts written by a\\r\\nuser. However, since the raw number of messages cannot be compared across\\r\\ndifferent intakes (maybe in an intake there are more topics), this value is represented\\r\\nas a percentage of messages written by the user over the total: 0 will\\r\\nmean that the student wrote 0% of the messages present in the forum, while 1\\r\\nwill mean that all the messages in the forum have been written by that student.\\r\\nLet Mc be represent the whole dataset of messages written in the discussion\\r\\nforum of course c (see examples in Table 4.12b), Mc;u \\x12 Mc the subset of\\r\\nmessages written by user u in the same course, and Mc;u;d \\x12 Mc;u the set\\r\\nof messages written not later than the Dth day of course, then the number of\\r\\nmessages for a user will be computed as\\r\\nnmuc;d =\\r\\njMc;u;dj\\r\\njMc;dj\\r\\n2 [0; 1] 8(u;D) 2 Uc \\x02 D 8c 2 C\\r\\n(b) received_replies While n_messages counts the written messages, this variable\\r\\naccounts for the received messages, that is the replies to one student\\x92s messages.\\r\\nAs already commented in previous sections, the number of replies was\\r\\nextracted and added as additional feature. By grouping the discussion posts\\r\\ndataset by user and summing the number of replies it is possible to compute\\r\\nthis value. However, since the number of replies tends to be proportional to the\\r\\nnumber of messages posted (the more you post the more probability you have\\r\\nto receive a reply), the value has been normalized dividing it by the number of\\r\\nmessages posted, becoming more the average number of replies received per\\r\\nposted message.\\r\\nLet Mu;d\\x14D be the set of messages written by user u no later than the Dth day\\r\\nof course. For each message m let mn_replies indicate the number of replies to\\r\\n51\\r\\nName Description\\r\\nA0.1 admission test score\\r\\ngrade_assg average grade in assignments (no quizzes)\\r\\ngrade_quiz average grade in quizzes\\r\\nn_messages percentage of messages in the forum written by the user\\r\\nreceived_replies average number of replies received per user per message written\\r\\nin the discussion forums\\r\\nmessage_length median length of messages posted in the discussion forums (#\\r\\nof chars)\\r\\nsentence_length median length of sentences used in messages posted in the\\r\\ndiscussion forums (# of chars)\\r\\nword_length median length of words used in messages posted in the discussion\\r\\nforums (# of chars)\\r\\nwords_ratio median percentage of words over tokens in discussion forums\\r\\nmessages\\r\\nuniquewords_ratio median percentage of unique words over words in discussion\\r\\nforums messages\\r\\nmessage_score median score of messages written in the discussion forums\\r\\ngiven_likes percentage of posts that were given a like by the user\\r\\nreceived_likes average number of likes received per post\\r\\nteacher_likes percentage of likes received by a teacher over total number of\\r\\nreceived likes\\r\\nread_posts percentage of posts read (over number of posts published)\\r\\nn_pr_comments percentage of comments in the peer reviews section written\\r\\nby the user\\r\\npr_comments_length median length of the comments written by the user in the peer\\r\\nreview section (# chars)\\r\\nn_views percentage of course contents viewed\\r\\nin_time_views percentage of contents viewed in the same week they were\\r\\npublished\\r\\nactive_days percentage of days of course when the student visited at least\\r\\none content\\r\\nconsumption_speed median time (in s) that a user takes to watch a content after its\\r\\npublication\\r\\nconsumption_time median time (in s) that a user spends on a content page\\r\\nTable 4.20: List of features extracted per each user from the datasets described in\\r\\nSection 4.1\\r\\nthat message. The number of received_replies will be then computed as\\r\\nrr(u;D) =\\r\\nP\\r\\nm2Mu;d\\x14D\\r\\nmn_replies\\r\\njMu;d\\x14Dj\\r\\n8(u;D) 2 U \\x02 D\\r\\n52\\r\\n(c) messages_length To compute this variable the median length of the messages\\r\\nis considered. Length here is intended as the number of single characters. Median\\r\\nhas been chosen over mean here and in other cases. Indeed, by looking at\\r\\nthe distribution of the variable (Figure 4.7) it happened to be pretty asymmetric,\\r\\nimplying that mean is more likely to be affected by extreme values than\\r\\nmedian.\\r\\nLet Mu;d\\x14D be the set of messages written by user u in the first D days of\\r\\ncourse. The variable messages length will be then computes as follows:\\r\\nml(u;D) = median\\r\\nm2Mu;d\\x14D\\r\\nmlength 8(u;D) 2 U \\x02 D\\r\\n(d) sentence_length It is the median length of all the sentences written by a student.\\r\\nIt is measured with the assumption that students who write longer sentences\\r\\nhave also better writing skills and possibly convey more complex and\\r\\ncomplete messages.\\r\\nLet Mu;d\\x14D the dataset of messages written by user u in the first D days of\\r\\ncourse. For each message m, let Sm be the set of sentences used in the message,\\r\\nand let Su;d\\x14D the set of all the sentences written by user u in all the\\r\\nmessages posted in the first D days of course. The sentence length will be\\r\\nthen computed as\\r\\nsl(u;D) = median\\r\\ns2Su;d\\x14D\\r\\nlen(s) 8(u;D) 2 U \\x02 D\\r\\n(e) word_length It is the median length of all the words written by a student.\\r\\nSimilarly to the case of sentence_length, also the length of words is taken into\\r\\naccount assuming that longer words identify students who make use of a more\\r\\nelaborate vocabulary, suggesting better written expression skills.\\r\\nLetMu;d\\x14D the list of messages written by user u in the first D days of course.\\r\\nFor each message m, let Wm be the list of words used in the message, and let\\r\\nWu;d\\x14D the list of all the words used by user u in all the messages posted in\\r\\nthe first D days of course. The word length will be then computed as\\r\\nwl(u;D) = median\\r\\nw2Wu;d\\x14D\\r\\nlen(w) 8(u;D) 2 U \\x02 D\\r\\n(f) words_ratio This variable takes into account the ratio of words over tokens.\\r\\nAs already mentioned, the term token is a widely accepted term in data mining\\r\\nand indicates any sequence of alphanumeric text (i.e. excluding white-spaces,\\r\\npunctuation, etc.). Although every token could be generally called \"a word\",\\r\\nfor the purpose of this project words here represent a subset of tokens composed\\r\\nby all those tokens that are not numbers nor stop-words (articles, prepositions,\\r\\ncommon verbs, etc.), that is a sort of more refined and meaningful\\r\\ntokens. The intuition is that the higher the ratio of words is, the better and the\\r\\nmore dense of information the message will be.\\r\\n53\\r\\nLet M be the set of all messages written. For each message m in this set, let\\r\\nTm Wm \\x12 Tm the two bags of tokens and words for the messages, respectively.\\r\\nThen, for each message the words-tokens ratio wrm will be\\r\\nwrm =\\r\\njWmj\\r\\njTmj\\r\\n2 [0; 1] 8m 2 M\\r\\nNow consider Mu;d\\x14D to be the subset of messages written by user u in the\\r\\nfirst D days of course. It is possible to compute wrm for every message m 2\\r\\nMu;d\\x14D and then take the median as such\\r\\nwtr(u;D) = median\\r\\nm2Mu;d\\x14D\\r\\nwrm 8(u;D) 2 U \\x02 D\\r\\n(g) uniquewords_ratio While the previous variable considered the percentage of\\r\\nwords over tokens, this variable considers the percentage of unique words over\\r\\nthe total number of words used. This value should give an idea of how elaborate\\r\\nthe message is. Is the student repeating a lot of words or looks for synonyms\\r\\nand uses a wide vocabulary?\\r\\nThe formulas are identical to the previous variable words_ratio. Just consider\\r\\nWm to be the list of words and Wm to be the actual set of words (that is,\\r\\nwithout repetitions); the variable will be then computed as\\r\\nuwr(u;D) = median\\r\\nm2Mu;d\\x14D\\r\\njWmj\\r\\njWmj\\r\\n8(u;D) 2 U \\x02 D\\r\\n(h) message_score It has been decided to give a score to each message to measure\\r\\nhow much the student is on topic with the concepts explained in the course.\\r\\nAlthough the scoring algorithm could constitute a topic on its own for another\\r\\nMaster\\x92s Thesis, to keep it simple and fit in the given time constraints, together\\r\\nwith teachers and project managers it has been decided to start with a simple\\r\\nweighted sum of all the words in a message. To make it possible, the teachers\\r\\nprovide a list of words that they consider to be relevant, each with its importance\\r\\nweight (Table 4.21).\\r\\nSince different teachers may pick up different ranges for their importance\\r\\nweights, once the teacher provides a list of words, each weight is replaced\\r\\nwith its proportion with respect to the total. That is, if zp\\r\\nx is the weight a professor\\r\\np gave to the generic word x, and W represent the total set of words in\\r\\nthe professor\\x92s list, the final weight zx will be calculated as follows\\r\\nzx =\\r\\nzp\\r\\nx\\r\\nmaxw2W zpw\\r\\nBy doing so, the total sum of the new weights is always equal to 1. Once\\r\\nthe weights are normalized, the message score msW (where W indicates the\\r\\nlist of words of the message) is simply computed as a sum of words weights.\\r\\n54\\r\\nEnglish Spanish Weight\\r\\nads anuncios 1\\r\\nallocate asignar 5\\r\\nallocation asignacion 5\\r\\nattribute atributo 3\\r\\nattribution atribucion 3\\r\\n...\\r\\ntotal total 1\\r\\nunbranded sin marca 5\\r\\nunique unico 1\\r\\nvisitors visitantes 2\\r\\nvisits visitas 1\\r\\nTable 4.21: Example of words importance list provided by teachers.\\r\\nHowever, exploratory data analysis showed that such a score is (as expected)\\r\\nvery highly correlated with the messages length. This means that with such\\r\\nformula, high message scores tend to be associated to long messages rather\\r\\nthan to content-dense messages. To solve this issue, the score is eventually\\r\\ndivided by the number of words in the message, thus resulting in an average\\r\\nscore per word. The formula of the message score for the generic message m\\r\\nwith a list of words Wm is\\r\\nmsm =\\r\\nP\\r\\nw2Wm zw\\r\\njWmj\\r\\n8m 2 M\\r\\n.\\r\\nIn this way, the message score could be ultimately thought about as the average\\r\\nword weight. Also, since the weights after normalization range between 0 and\\r\\n1, also the score ranges between 0 and 1, where 0 means that none of the\\r\\nwords used was considered relevant by the teacher and 1 that each word used\\r\\nwas among the most relevant ones.\\r\\nTo produce a value for a given user and a number of days, the median score of\\r\\nall messages written by the user in the defined time interval is considered, like\\r\\nso\\r\\nms(u;D) = median\\r\\nm2Mu;d\\x14D\\r\\nmsm 8(u;D) 2 U \\x02 D\\r\\n(i) read_posts This variable considers the number of posts read by each user.\\r\\nThe intuition behind it is that the students who read more messages know also\\r\\nmore. However, since the raw number of read posts is related to the total number\\r\\nof messages posted in the discussion forum of one course (the more posts\\r\\nwritten the more posts are there to be read), this value is normalized by the\\r\\ntotal number of posted messages in the discussion forum of the corresponding\\r\\ncourse.\\r\\n55\\r\\nConsider the dataset of unread posts (Table 4.19b): let Uc be the set of all users\\r\\nattending course c, Tc the complete set of discussion topics in course c and ntc\\r\\nthe total number of posts for the topic t (belonging to course c), while ytc;u the\\r\\nnumber of unread posts in topic tc by user uc. The number of read posts rp for\\r\\nthe user u will then be:\\r\\nrpu =\\r\\nP\\r\\ntc2Tc (ntc ?? ytc;u) P\\r\\ntc2Tc ntc\\r\\n8u 2 Uc; 8c 2 C\\r\\nUnfortunately, in this case it is not possible to compute the variable for an arbitrary\\r\\nnumber of days because it was not possible to download daily information\\r\\nabout the number of posts read.\\r\\nFigure 4.7: Distribution for some features related to students\\x92 writing skills. Since all of\\r\\nthem happen to be quite skewed due to extreme values, median has been preferred over\\r\\nmean to aggregate the values of a same student.\\r\\n(j) received_likes This variable should give an idea of how many likes a student\\r\\nreceived. Since the number of likes received depends on the number of posts\\r\\npublished, to reduce the correlation between number of posts and number of\\r\\nlikes received this last number has been normalized dividing it by the total\\r\\nnumber of posts written by the given user. In this way the variable actually\\r\\ntells the average number of likes received per message posted.\\r\\nLet M be the ratings dataset, with one row per posted message (as shown in\\r\\nTable 4.19a). Since the ratings dataset has information about the author and the\\r\\n56\\r\\ndate of each message, letMu;d\\x14D \\x12 Mbe the portion of dataset with messages\\r\\nwritten by user u within the first D days of course.\\r\\nFor the generic message m, let mreceived_likes be the total number of likes received.\\r\\nThe number of received likes rl for a generic user u is then computed\\r\\nas the sum of likes to message written by user u over the total number of messages\\r\\nwritten by him, like so:\\r\\nrl(u;D) =\\r\\nP\\r\\nm2Mu;d\\x14D\\r\\nmreceived_likes\\r\\njMu;d\\x14Dj\\r\\n8(u;D) 2 U \\x02 D\\r\\n(k) teacher_likes This feature originates from the assumption that the like of a\\r\\nteacher is more relevant and gives more information about the student than\\r\\na like from a peer. The idea is that if a student receives many likes from a\\r\\nteacher, probably he is learning and performing well. Thus, the need to take\\r\\ninto account teachers\\x92 likes, that is likes received by a teacher. Yet, since the\\r\\nnumber of likes received by teachers is strictly correlated with the number of\\r\\nreceived likes, the number is normalized by taking their ratio. In this way, this\\r\\nfeature can be interpreted as the percentage of likes received by the teacher\\r\\nover the total number of received likes.\\r\\nWith the ratings dataset shown in Table 4.19a it is possible to count the likes\\r\\nreceived by a teacher per each message. This is possible by filtering out all the\\r\\ncolumns of users that are not teachers, then adding up the likes over the same\\r\\nrow. Let this number be denoted by mteacher_received_likes for each message m.\\r\\nThen the number of teacher likes tl will be computed as follows:\\r\\ntl(u;D) =\\r\\nX\\r\\nm2Mu;d\\x14D\\r\\nmteacher_received_likes\\r\\nmreceived_likes\\r\\n8(u;D) 2 U \\x02 D\\r\\n(l) given_likes This variable should give an idea of how many posts a given user\\r\\nrates. Since the number of likes are correlated with the total number of messages\\r\\nposted in the forum, this correlation is removed by taking the ratio between\\r\\nthe two values. In this way the variable is a percentage of available posts\\r\\nrated by the user. The idea is that the more likes given, the higher is the interest\\r\\nand engagement in the discussions and in the course topics.\\r\\nLetMd be the slice of ratings dataset (see Table 4.19a) containing all the messages\\r\\nposted in the first D days of course. Let lm;u denote the number of likes\\r\\ngiven by user u to message m. The number of given likes for user u and D\\r\\ndays will be then computed as\\r\\ngl(u;D) =\\r\\nP\\r\\nm2Mu;d\\x14D\\r\\nlm;u\\r\\njMu;d\\x14Dj\\r\\n8(u;D) 2 U \\x02 D\\r\\n(m) n_pr_comments This variable is very similar to n_messages. The only difference\\r\\nis that instead of counting the messages in the forum, it counts the\\r\\ncomments to the peer reviews.\\r\\n57\\r\\n(n) pr_comments_length This variable is very similar to message_length. The\\r\\nonly difference is that instead of considering the length of the messages in the\\r\\ndiscussion forum, it considers the length of comments to the peer reviews.\\r\\n3. Learning Resources\\r\\n(a) n_views This variable measures the number of items (contents) visited by a\\r\\nstudent. Since the same content can be viewed multiple times, it has been\\r\\ndecided to count the number of unique items viewed at least once. Since in\\r\\ndifferent intakes the total number of released items might change, the number\\r\\nof unique items visited is divided by the total number of available items. Over\\r\\nan arbitrary interval of time of D days the variable is computed simply as the\\r\\nfraction of unique contents viewed before that day divided by the total number\\r\\nof items unlocked so far.\\r\\nLet Id be the set of items published no later than day D of course, while\\r\\nJu;d\\x14D the dataset of items published and viewed in the same time interval by\\r\\nthe user u. Now remove in Ju;d\\x14D views that refer to the same item to get\\r\\na set containing only unique items viewed, say Ju;d\\x14D. The variable is then\\r\\ncomputed as follows:\\r\\nnw(u;D) =\\r\\njJu;d\\x14Dj\\r\\njIdj\\r\\n8(u;D) 2 U \\x02 D\\r\\n(b) in_time_views The idea of this variable is to measure how many of the contents\\r\\nviewed are actually viewed on time. Since each item view has the date\\r\\nof publication of the content (unlock_at) and the date of the visualization (created_\\r\\nat), it is possible to compute also the corresponding weeks of course, say\\r\\ncreated_at_woc and unlock_at_woc, respectively.\\r\\nThe views to be considered on time should be all the views for which created_\\r\\nat_woc and unlock_at_woc match. However, let us think about a user\\r\\nwho generally watches all contents on time and he is so diligent that re-watches\\r\\nsome lectures after time just to refresh some concepts. In this case, all these\\r\\nlate views would be considered as not on time even though they are not an indicator\\r\\nof poor attention but quite the opposite. For this reason, the item views\\r\\nare immediately filtered to keep for each user and item, only the first view in\\r\\nchronological order. Also, since the number of views on time is correlated\\r\\nwith the total number of views, the ratio between these two values is taken.\\r\\nThe variable can be then interpreted as the percentage of items viewed on time\\r\\n(the first time) over the total number of items viewed (at least once).\\r\\nNow, let J0\\r\\nu;d\\x14D be the filtered dataset containing only the first view per each\\r\\nitem viewed by user u in the firstD days of course, while J0intime\\r\\nu;d\\x14D = J0\\r\\nu;d;(created_at_woc==unlock_at_wocJ0\\r\\nu;d\\x14D the subset of items viewed on time. The variable will be then computed\\r\\nas\\r\\nitv(u;D) =\\r\\njJ0intime\\r\\nu;d\\x14Dj\\r\\njJ0\\r\\nu;d\\x14Dj\\r\\n2 [0; 1] 8(u;D) 2 U \\x02 D\\r\\n58\\r\\nThis ratio should be equal to 0 for a student who always watches the material\\r\\nlate, while it should be closer and closer to 1 as the student watches everything\\r\\non time.\\r\\n4. Time on Platform\\r\\n(a) active_days Using once again the item views dataset is also possible to have\\r\\nan estimation of the number of days that a user was active. For the purpose\\r\\nof this project it has been assumed that viewing at least one content can be\\r\\nconsidered as a day of activity. Because of this, the number of active days is\\r\\nthe number of days when at least one item was viewed. To make this variable\\r\\na function of time, the number of active days is divided by the total number of\\r\\ndays considered. For example, the active_days in the first 10 days of course\\r\\nwill be computed considering only the subset of item views that took place in\\r\\nthe first 10 days of course and dividing the number of days with at least one\\r\\nview by 10. In this way the variable actually represents a fraction of active\\r\\ndays: it is 1 if the student watched at least one content every day of course,\\r\\nwhile it is 0 if the user did not watch any content at all.\\r\\nConsider the item views dataset J and the created_at field, which gives information\\r\\nabout the time-stamp when the item was viewed. Now, for each\\r\\ntimestamp extract the date as a new field, say created_at_date. Now consider\\r\\nthis column Jcreated_at_date. By removing the duplicates it is possible to create a\\r\\nset of dates when at least one content was viewed. The variable will be then\\r\\ncomputed as\\r\\nad(u;D) =\\r\\njfd : d 2 Jcreated_at_dategj\\r\\nD\\r\\n8(u;D) 2 U \\x02 D\\r\\n(b) consumption_time This is an indicator of how much does a student spend on\\r\\none content before switching page. To do this, the dataset considered was the\\r\\nwhole page views dataset P. For each student, the page views were ordered\\r\\nchronologically. Then, in case of items views (recall that item views are a\\r\\nsubset of page views), the difference of time (in seconds) between the item\\r\\nview and the subsequent page view was taken.\\r\\nLet P a set of chronologically ordered page views, so that the generic page\\r\\nview p is followed by the page view q = p + 1. Also, let J \\x12 P the subset\\r\\nof page views that are also item views. For every page p, let tp the creation\\r\\ntime-stamp (i.e, the time when it was viewed). Then the consumption time of\\r\\nthe page ctp will be:\\r\\nctp =\\r\\n(\\r\\nNaN if p =2 J\\r\\ntp+1 ?? tp otherwise\\r\\nNow consider Pu;d\\x14D to be the pages viewed by user u in the first D days of\\r\\ncourse. The final variable is computed as the median value of the consumption\\r\\n59\\r\\ntimes of such pages, like so:\\r\\nct(u;D) = median\\r\\np2Pu;d\\x14D\\r\\nctp 8(u;D) 2 U \\x02 D\\r\\nMedian has been preferred over mean because of the asymmetric distribution\\r\\nof the variable (see Figure 4.8), since median is more stable and less influenced\\r\\nby extreme values.\\r\\n(c) consumption_speed This variable takes into account how fast does a student\\r\\nconsume the learning material available by considering the difference between\\r\\nthe consumption time-stamp (created_at) and the publication time-stamp (unlock_\\r\\nat). Computing how much time has passed between the publication of\\r\\nan item and its consumption (consumption delay) is pretty straightforward in\\r\\nthe items views dataset because each item view has both these fields. Once\\r\\nthis time difference has been computed for each item view, it is possible to\\r\\ngroup this value by user and compute the median. Also in this case the median\\r\\nproved to be the more stable measure and it has been chosen.\\r\\nFor each page p let the consumption delay cdp be equal to\\r\\ncdp = pcreated_at ?? punlock_at\\r\\nNow consider Pu;d\\x14D to be the pages viewed by user u in the first D days of\\r\\ncourse. The final consumption delay is computed as the median value of the\\r\\nconsumption delays of such pages. like so:\\r\\ncd(u;D) = median\\r\\np2Pu;d\\x14D\\r\\ncdp 8(u;D) 2 U \\x02 D\\r\\nIt may be noted that consumption delay is high for poorly performing students\\r\\nwhile for all the other variables high values are associated to good, positive\\r\\nbehaviours. To keep this semantics, the consumption delay is turned upside\\r\\ndown and converted in consumption \"speed\". The idea is to take the difference\\r\\nbetween the maximum time available to watch a content (i.e. the total\\r\\nnumber of seconds in the considered number of days) and students\\x92 median\\r\\nconsumption delay. This means that the consumption speed will be computed\\r\\nas\\r\\ncs(u;D) = (D \\x01 24 \\x01 60 \\x01 60) ?? cd(u;D) 8(u;D) 2 U \\x02 D\\r\\nThe reader may notice that although in previous sections it was told that ratings\\r\\ncould not be computed as a function of time, her they actually are. Indeed, the\\r\\nsolution adopted is a work-around: the received likes should be the number of\\r\\nreceived likes before a given day, but it has been transformed to number of received\\r\\nlikes to messages posted before a given day; similarly, the given likes should be the\\r\\nnumber of likes given in a interval of time, but it has been transformed to number of\\r\\nlikes given to messages posted in that same interval. Although this is a temporary\\r\\nsolution to approximate the number in absence of proper data, it must be highlighted\\r\\nthat it is not the same.\\r\\n60\\r\\nFigure 4.8: Distribution for some features related to time-on-task. Since all features\\r\\nhappen to be quite skewed due to extreme values, median has been preferred over mean.\\r\\nStudents\\x92 Final Grades\\r\\nTo conclude this section about students features, the only information missing are the\\r\\nlabels, that is the final grades. Student final grades are computed automatically within\\r\\nCanvas with a mathematical formula that takes into account assignments and quizzes\\r\\ngrades. To understand how they are computed, recall from Section 4.1 that each assignment\\r\\nor quiz belongs to a so-called assignment group which in turn has an importance\\r\\ncalled group weight. Also, each assignment has its own maximum score (called points\\r\\npossible), while each user submission is graded with a number of points (named points\\r\\nscored).\\r\\nNow let A be the whole set of assignments and G the set of assignment groups. Let\\r\\nag denote an assignment ag 2 A which belongs to the assignment group g 2 G. Then\\r\\nlet pua\\r\\nbe the number of points scored in assignment a by a student u, qa the total points\\r\\npossible for assignment a, while wg the group weight of group g. The final score s of a\\r\\nuser is given by the weighted sum of the proportions of points scored over points possible\\r\\nin each group, that is:\\r\\ns(u) =\\r\\nX\\r\\ng2G\\r\\nwg \\x01\\r\\nP\\r\\na2A pua\\r\\nP\\r\\na2A qa\\r\\n2 [0; 1] 8u 2 U\\r\\nGiven the continuous nature of this variable, it had to be discretized to fit to the classification\\r\\nmodeling planned in Section 1.2.2. However, discretizing grades is not that\\r\\nstraightforward: there are many possible ways to discretize them, and if the underlying\\r\\ndistribution is asymmetric (see Figure 4.9) most of these strategies may generate very\\r\\nimbalanced classes.\\r\\n61\\r\\nIn case of school grades, the most reasonable ways to discretize them are two:\\r\\n1. binary, between passing vs failing grades, that is grades that allow students to\\r\\nsuccessfully complete the course (>= 0.6) and grades that imply a failure (< 0.6).\\r\\nLooking at the balance of these two classes (Figure 4.9b) it is possible to see a clear\\r\\ndisproportion between the number of passing and failing students: in the intakes\\r\\nunder analysis most of the students have passed the course, while failure rate is very\\r\\nlow (luckily, someone would say). Because of this, this discretization approach can\\r\\nbe used for the independent t-Test planned in Section 1.2.2 (DG.4.), but is suboptimal\\r\\nfor the multi-class classification problem (DG.5.).\\r\\n2. multi-class, with letter grades, which are one of the most widespread ways to\\r\\ndiscretize grades. In case of HIOPs the proposed grading scale has 12 classes (see\\r\\nTable 4.22): A, A+, B+, B, B-, C+, C, C-, D+, D, D-, F. Being 12 a too high number,\\r\\na wiser solution is to reduce the number down to 5 by considering only the actual\\r\\nletters, without additional signs. Yet, looking at the size of the classes (Figure 4.9c),\\r\\nalso this discretization seem to generate pretty imbalanced classes.\\r\\n(a) (b)\\r\\n(c) (d)\\r\\nFigure 4.9: Students\\x92 grades distribution according to different discretization\\r\\napproaches: decimal (a), binary (b), 5-letters (c). To try to solve their balance issues, the\\r\\nultimate strategy uses 4 classes (d).\\r\\n62\\r\\nGrade 12-Letters 5-Letters Binary\\r\\n0.95 A A\\r\\nPass\\r\\n0.90 A-\\r\\n0.85 B+\\r\\n0.80 B B\\r\\n0.75 B-\\r\\n0.70 C+\\r\\n0.65 C C\\r\\n0.60 C-\\r\\n0.55 D+\\r\\nD Fail 0.50 D\\r\\n0.45 D-\\r\\n0.00 F F\\r\\nTable 4.22: Grading schemas. The classes that have been eventualy chosen to discretize\\r\\nstudents\\x92 grades are highlighted with bold text.\\r\\nAs a matter of fact, it must be also noted that to choose a strategy balancing is important\\r\\nbut is not everything: consider, for example, to divide grades into two classes, those\\r\\nbelow or equal to C on one side and those above C on the other. This strategy seems\\r\\nto reach the best balance. However, this strategy would be of little practical relevance,\\r\\nconfusing and hard to interpret: is a professor interested in knowing if a student will score\\r\\nmore or less than C? If a student is predicted with the low-scoring class, should the teacher\\r\\nbe worried? Maybe the student is expected to score 0.65 and pass, maybe 0.2 and fail,\\r\\nbut they would require the same level of attention. Because of this, a third slightly worsebalanced\\r\\nbut more meaningful approach is proposed: four classes, the two failing letter\\r\\ngrades together (D and F), plus the three passing letter grades. In this case the model will\\r\\nbe trained to distinguish between passing (D and F) and non-passing students, and among\\r\\nthe letter, to distinguish between high (A), average (B) and low (C) performing students.\\r\\nFigure 4.9d shows the ultimate labels repartition: A, B, C, F, where now F stands for\\r\\nFailing grade. Although labels are still quite imbalanced, re-sampling will be discussed\\r\\nafter pre-processing (because the introduction of new synthetic samples may affect some\\r\\nstatistics such as variance and correlation).\\r\\n4.2.2 Modeling Datasets Generation\\r\\nWith the features just described it is possible to generate all the different datasets needed\\r\\nfor modeling. Recall from Section 1.2 that the main problems to need modeling are two:\\r\\nknowledge and engagement features importance extraction and students\\x92 performance\\r\\nprediction.\\r\\n\\x95 K&E Features Importance\\r\\n63\\r\\nFor this first problem, the idea is to derive from all the most recent data which\\r\\nis the importance of each feature when defining the two predefined concepts of\\r\\nknowledge and engagement. Because of this, all the features are computed over\\r\\nthe total duration of the course (35 days), then the resulting dataset is split into two\\r\\ndistinct sub-datasets based on the features: all the features that are considered to\\r\\naccount for knowledge in one dataset and all the engagement features in the other.\\r\\nWhere possible, these two datasets will be described as if they were one single\\r\\ndataset with 35-days features. Also, it must be noted that nor knowledge nor engagement\\r\\nare labelled: in fact, no labels are available.\\r\\n\\x95 Students\\x92 Performance Prediction\\r\\nFor the second problem, the features over 7, 14, 21 and 28 days have been used to\\r\\ngenerate four different datasets. To understand why, recall that the goal is to learn\\r\\nto predict students\\x92 final success so to perform early interventions on students that\\r\\nare predicted to have bad performance. Since it makes no sense to predict the final\\r\\ngrade of a student once the course has already finished nor during the last week\\r\\n(when there is no more room for any intervention), it is clear that the models must\\r\\nbe trained with \"early data\". Considering that courses are 5 weeks long but the last\\r\\none is not usable for prediction, it has been decided to train four different models,\\r\\none for each previous week. In this case there is no distinction between knowledge\\r\\nand engagement and all the features are considered together.\\r\\nTrain-Test Split\\r\\nBefore proceeding further, it is important to recall that data preprocessing must be performed\\r\\non training data solely. Because of this, before performing any of the preprocessing\\r\\nsteps described in the following section, all the datasets just mentioned have been split\\r\\ninto 80% of training data and 20% of testing data.\\r\\nWhile the 35-days dataset has been randomly split between training and test data, in\\r\\nthe case of the datasets for performance prediction, given the precarious balance between\\r\\nthe classes stratified sampling has been chosen to preserve the same proportion of labels\\r\\nin the two datasets and ensure the presence of all classes in the test set.\\r\\nHaving a total of 66 students in each dataset, this leaves 52 students for training and\\r\\nvalidation and only 14 for testing.\\r\\n4.2.3 Data Preprocessing\\r\\nOnce it is clear how to compute all the indicators and generate all the datasets needed,\\r\\ndata must be still explored and cleaned from missing values or useless features. Given\\r\\nthe large number of generated datasets, it is unworkable to discuss all of them in detail.\\r\\nActually, the purpose of this section is rather to present the general rules applied to clean\\r\\nany of these datasets. Cleaning has been divided into three main phases: missing values\\r\\ntreatment, dimensionality reduction and features scaling.\\r\\nPlease notice that all preprocessing is performed exclusively with training data. This\\r\\nis because test data are not meant to be known in any step except final testing. The\\r\\n64\\r\\ntransformations outlined in this chapter are applied later to test data using training data\\r\\ninformation still (by means of example, z-score normalization would be applied to test\\r\\ndata using training data mean and variance).\\r\\nMissing Values Treatment\\r\\nThe first step in data cleaning is to handle missing values. Figure 4.10 gives a visual\\r\\nrepresentation of missing values in the various datasets used for students\\x92 performance\\r\\nprediction. In these figures it is possible to notice few interesting things:\\r\\n\\x95 In the case of 7-days and 14-days datasets, the two features relative to peer reviews\\r\\nare null for all the students. That is because peer reviews are done only starting\\r\\nfrom week 3. More in general, columns with 100% of missing values are dropped;\\r\\n\\x95 Speaking about items and page views features, in the 7-days dataset there are few\\r\\nstudents who still have null fields, most probably due to temporary inactivity. Because\\r\\nof it, these fields are filled with zeros when missing;\\r\\n\\x95 There are users for which all the discussion forums and/or peer reviews are null:\\r\\nthese students are those who never posted a message in the forum and/or a comment\\r\\nin the peer reviews section. Since they still represent a minority, they have been kept\\r\\nand all the corresponding values have been filled with zeros;\\r\\n\\x95 Also for grades like for item views, some fields are missing for the earliest datasets,\\r\\nmeaning that some students may have started the course later. Also in this case all\\r\\nthe fields have been filled with zeros.\\r\\nDimensionality Reduction: Low-Variance Filter & High-Correlation Filter\\r\\nTo reduce the number of features, low-variance and high-correlation filters have been\\r\\napplied. Please notice that dimensionality reduction is applied only to the datasets for\\r\\nperformance prediction; in case of the dataset for engagement and knowledge features\\r\\nextraction all the features have been kept not to lose any piece of information at the moment\\r\\nof dividing up students into groups and also because a weight of importance is\\r\\nrequired for any feature selected.\\r\\nVariance is used because low-variance features encode less information and in some\\r\\ncases tend to have less predictive power: taking it to the extreme, if variance is zero then\\r\\nthe feature is constant and clearly have no predictive power whatsoever. Yet, it must be\\r\\nalso acknowledged that extreme scenarios were this assumption does not hold may still\\r\\nexist: think about the case of a dataset with two classes and a variable whose value is\\r\\nalways equal to 0.49999 for one class and 0.50001 for the other. Its variance would be\\r\\nminimal, yet it would be of paramount importance for classification purposes. In any\\r\\ncase, since variance depends on the magnitude of the values range, variance must be\\r\\ncomputed after having min-max normalized data (clearly z-score is not an option here).\\r\\nThe variance for the features after normalization can be found in Table 4.23. 0.02 has\\r\\nbeen chosen as threshold value below which a variable is considered irrelevant; however,\\r\\n65\\r\\nFigure 4.10: Missing values heat-maps for the 4 generated datasets for students\\x92\\r\\nperformance prediction. Missing values are represented with black cells. It is possible to\\r\\nsee how the number of missing values decreases with time.\\r\\nno features have been found to be below the threshold so no variables have been dropped\\r\\nin any of the datasets.\\r\\nAlso correlation is used in feature selection because highly-correlated features tend to\\r\\nhave the same information and are thus redundant. 0.8 already indicates a strong linear\\r\\nrelation and it has been considered as a threshold.\\r\\nFeatures Scaling\\r\\nSince the features have all different ranges, are expressed in different units, it is wise to\\r\\nscale the data before moving to modeling. Scaling is a procedure indicated for or required\\r\\n66\\r\\n7 14 21 28 35\\r\\ngrade_admission 0.0396 0.0396 0.0396 0.0396 0.0396\\r\\nactive_days 0.0768 0.0614 0.0509 0.0423 0.0455\\r\\nconsumption_speed 0.0387 0.0267 0.0317 0.0329 0.0292\\r\\nconsumption_time 0.0570 0.0275 0.0461 0.0292 0.0341\\r\\ngiven_likes 0.0733 0.0594 0.0392 0.0362 0.0363\\r\\ngrade_assg 0.0309 0.0254 0.0256 0.0268 0.0236\\r\\ngrade_quiz 0.0677 0.0471 0.0305 0.0303 0.0303\\r\\nin_time_views 0.0441 0.0558 0.0592 0.0530 0.0509\\r\\nmessage_score 0.0730 0.0542 0.0727 0.0632 0.0562\\r\\nmessages_length 0.0528 0.0504 0.0384 0.0480 0.0359\\r\\nn_messages 0.0584 0.0384 0.0568 0.0430 0.0378\\r\\nn_pr_comments \\x97 \\x97 0.0763 0.0577 0.0380\\r\\nn_views 0.0845 0.0588 0.0490 0.0475 0.0505\\r\\npr_comments_length \\x97 \\x97 0.0586 0.0461 0.0355\\r\\nreceived_likes 0.0553 0.0507 0.0701 0.0576 0.0466\\r\\nreceived_replies 0.0778 0.0447 0.0307 0.0272 0.0311\\r\\nsentence_length 0.0976 0.0589 0.0624 0.0503 0.0464\\r\\nteacher_likes 0.2105 0.1514 0.1213 0.1140 0.1063\\r\\nuniquewords_ratio 0.1254 0.0827 0.0698 0.0679 0.0658\\r\\nword_length 0.1211 0.0945 0.0634 0.0534 0.0534\\r\\nwords_ratio 0.1369 0.1001 0.0842 0.0775 0.0767\\r\\nTable 4.23: Features variance for all the 5 generated datasets. Since all values are above\\r\\nthe defined threshold (0.02), no feature was removed.\\r\\nFigure 4.11: Features correlation heat-maps for the 7-days and the 35-days datasets.\\r\\nby many machine learning algorithm and its goal is to transform features so to make them\\r\\nsomehow comparable in terms of ranges, distributions and weights. The most common\\r\\n67\\r\\nscaling techniques are min-max scaling and standardization (z-score). According to literature,\\r\\nthe former is more indicated in cases when data do not necessarily follow a normal\\r\\n(Gaussian) distribution, while the latter assumes normality. Both, however, are not robust\\r\\nto outliers or extreme values in general.\\r\\nIn the generated datasets, while normality could be assumed for many features, it\\r\\nis also true that there are some other features whose distribution is very much skewed\\r\\nand with long tails, thus with very extreme values. Hence data have been eventually\\r\\ntransformed using a third scaling technique, more robust to outliers: interquartile scaling\\r\\n(sometimes called robust scaling).\\r\\nTo prove its benefits, Figure 4.12a shows the comparison of these scaling techniques\\r\\nfor two variables computed after 7 days that originally range in the same interval ([0; 1])\\r\\nbut have very different distributions: grade_assg (in black), very skewed to high grades\\r\\nand with few examples of students who did not complete any assignment yet (left tail),\\r\\nand active_days (gray-colored) which instead seems to be pretty normally distributed. It\\r\\nis possible to see how interquartile scaling is indeed robust to outliers: while with minmax\\r\\nand standard scaling the distributions are still very different, with the third technique\\r\\nthe distributions almost overlap. Also, it must be noted that robust scaling works great\\r\\nalso without outliers: Figure 4.12b shows the same comparison with the same features but\\r\\ncomputed over 35 days, when the outliers in grade_assg are now absent; in this case, with\\r\\nnormal distributions standard scaling works great as expected, but robust scaling proves\\r\\nto be even better also in this case.\\r\\n4.3 Modeling & Evaluation\\r\\nOnce all the necessary datasets have been cleaned and preprocessed, in this section they\\r\\nare finally used to fulfil the data mining and business goals defined in Section 1.2. The\\r\\nmacro-problems that need to be solved are three: first, the identification of key differences\\r\\nin the behaviour and the activity of differently performing groups of students; then the\\r\\nextraction of knowledge and engagement features importance (Section 4.3.2; finally, the\\r\\nprediction of students\\x92 final grade from early activity in the course (Section 4.3.3).\\r\\n4.3.1 Independent t-Test for Differently Performing Groups of\\r\\nStudents\\r\\nCourse managers are interested which are the main differences between differently performing\\r\\nstudents. More specifically, four pair of groups are considered of interest:\\r\\n1. A-scoring students and B-scoring students;\\r\\n2. B-scoring students and C-scoring students;\\r\\n3. C-scoring students and failing students (D+F);\\r\\n4. passing students (A+B+C) and failing students (D+F).\\r\\nTo answer these questions, Student\\x92s t-Test is used. Student\\x92s t-Test is a statistical test\\r\\nthat validates the hypothesis that the mean value of a variable for two independent groups\\r\\n68\\r\\n(a)\\r\\n(b)\\r\\nFigure 4.12: Comparison of different scaling techniques for two variables (grade_assg\\r\\nand active_days) once computed over 7 days (a), then over 35 (b).\\r\\nis equal. Since t-Test has different definitions depending whether the variable for the two\\r\\ngroups has same variance or not, prior to the t-Test for equal mean, a Levene test for equal\\r\\nvariance is performed. In both cases the null hypothesis is rejected for p-values smaller\\r\\nthan 0.025.\\r\\nPassing Students (A+B+C) vs Failing Students (D+F)\\r\\nThis test is meant to identify which features most differ between the students that succeed\\r\\nin the course and those who do not. From Table 4.24 it emerges that the main differences\\r\\nbetween these two groups are numerous:\\r\\n1. speaking about overall activity, passing students tend to be active 52% of the days\\r\\nof course and watch more than 80% of the contents available, while failing students\\r\\nare active only 33% of the days and view less than 60% of the contents;\\r\\n2. in the discussion forums and in the peer reviews section passing students are more\\r\\nactive as they write 10% of the messages against 4-5% of failing students. Also,\\r\\nin the forum the students who pass receive on an average 1.14 likes per message,\\r\\nwhile failing students only 0.7;\\r\\n69\\r\\n3. the quality of writing is different, too: passing students write longer words, longer\\r\\npeer review comments and more significant words (every 10 tokens, passing students\\r\\nhave used 7 words, failing students only 4).\\r\\nMean (Pass) Mean (Fail) Var (Pass) Var (Fail) Equal Var t-Test p-value\\r\\nactive_days 0.521 0.333 0.020 0.042 True 0.000\\r\\nwords_ratio 0.702 0.418 0.052 0.078 True 0.000\\r\\nn_pr_comments 0.102 0.043 0.003 0.002 True 0.000\\r\\nword_length 6.287 4.583 1.072 7.720 True 0.001\\r\\nreceived_likes 1.136 0.702 0.137 0.275 True 0.001\\r\\nn_messages 0.100 0.049 0.003 0.003 True 0.007\\r\\nn_views 0.820 0.565 0.028 0.094 False 0.016\\r\\npr_comments_length 319.148 96.292 102699.100 9245.657 True 0.021\\r\\nTable 4.24: Passing vs Failing students independent t-Test most significant results.\\r\\nA-scoring Students and B-scoring Students\\r\\nThis test is meant to see which are the key differences, apart from assignments and quiz\\r\\ngrades, between excellent and good students. As expected, the number of differences is\\r\\nmuch reduced. In fact, there is only one variable for which the t-Test had to reject the null\\r\\nhypothesis: the admission test, with A students scoring on average 23.3 points over 100,\\r\\nwhile B students only 11.4 points.\\r\\nB-scoring Students and C-scoring Students\\r\\nIn this case the focus is on the difference between high-performing students and lowperforming\\r\\nstudents. Interestingly, also in this case the variable for which the test rejected\\r\\nequal mean is only one: message_score, which accounts for the median of the average\\r\\nword value across all messages. However, the result goes against the intuition: the value\\r\\nis higher for C-students than for B-students (0.131 vs 0.072).\\r\\nC-scoring Students vs Failing Students (D+F)\\r\\nFinally, the last comparison of interest is between the low-scoring students and the failing\\r\\nones. Unfortunately, also in this case the t-Test did not provide interesting insights: the\\r\\nonly relevant variable is the number of peer reviews comments, with low-scoring students\\r\\nwriting on average 9% of the comments, while failing students only 4%.\\r\\n4.3.2 K&E Features Importance\\r\\nThe question to be answered here is: which are the indicators that maximize students\\x92\\r\\nknowledge and engagement?\\r\\nThe first step to answer these question is to define what is knowledge and engagement.\\r\\nBecause of this, the features defined in Section 4.2 have been divided into two subsets,\\r\\n70\\r\\nKnowledge Engagement\\r\\ngrade_admission active_days\\r\\ngrade_assg in_time_views\\r\\ngrade_quiz consumption_speed\\r\\nn_views n_messages\\r\\nconsumption_time messages_length\\r\\nreceived_likes n_pr_comments\\r\\nteacher_likes pr_comments_length\\r\\nreceived_replies given_likes\\r\\nmessage_score\\r\\nuniquewords_ratio\\r\\nwords_ratio\\r\\nword_length\\r\\nsentence_length\\r\\nTable 4.25: Repartition of extracted features between knowledge and engagement.\\r\\neach containing the indicators to account for the corresponding concept. The attribution of\\r\\na feature to one or the other subset is arbitrary: to do this, common sense and the opinion\\r\\nof the academic staff at IE was used. The repartition of features between knowledge\\r\\nand engagement is shown in Table 4.25. Yet, once distinct knowledge and engagement\\r\\ndatasets have been created, there is still one pending issue: they are not labelled. How to\\r\\nfind the elements that maximize these two concepts, then?\\r\\nThe solution proposed is the following:\\r\\n1. create labels from the data, either by clustering or by arbitrary choice;\\r\\n2. train (and test) a bag of models so to select the best performing one;\\r\\n3. extract features importance from the chosen model.\\r\\nLabelling: Strategies and Definition\\r\\nThe first strategy to create new labels was to use clustering. To do this, data were firstly\\r\\nscaled using the robust scaling technique described in Section 3.2, then silhouette score\\r\\nhas been computed and compared for different number of clusters (2 to 7) and for two\\r\\ndifferent clustering algorithms (KMeans[29] and Hierachical Clusering[41]). Recall from\\r\\nSection 3.2 that the silhouette score (also called silhouette coefficient) is a measure of\\r\\ncluster quality: it ranges between -1 (worst scenario, it means that samples are possibly\\r\\nassigned to wrong clusters, as a different cluster is more similar) and 1 (best scenario,\\r\\nperfect clustering); 0 means that clusters are possibly overlapping.\\r\\nThe results obtained for engagement and knowledge datasets are shown in the plots on\\r\\nthe left side of Figure 4.13a. It is possible to see that the two algorithms behave more or\\r\\nless the same way. However, while for knowledge the best number of clusters seems to be\\r\\n2, for engagement the judgement is not so straightforward and all the silhouette score are\\r\\npretty low. Also, looking at the classes balance (Figure 4.13b, right side) the clusters are\\r\\nreally imbalanced, probably explaining the high silhouette score: few samples are really\\r\\n71\\r\\n(a)\\r\\n(b)\\r\\nFigure 4.13: Clusters Quality Analysis. On the top (a), silhouette score and clusters\\r\\nbalance (using agglomerative/hierarchical clustering) for the engagement features, on the\\r\\nbottom (b) for the knowledge features. Although k = 2 seems to be the best option\\r\\nlooking at the silhouette scores, the repartition of the samples between the clusters is\\r\\ntotally unbalanced.\\r\\ndistant from the rest of population (almost outliers), while the rest is more homogeneous.\\r\\nAfter taking these elements into consideration it was decided to discard this option and\\r\\nchange approach.\\r\\nThe second strategy was to create labels by diving students into two groups using the\\r\\nvalues of the z-score of their features. Since the z-score gives an idea of how much a value\\r\\ndeviates (below or above) from the mean, it is possible to take an average of the z-scores\\r\\nof all the features of a student and say that he is generally above or below the average\\r\\nof the students looking whether the average z-score of his features is greater or less than\\r\\n0. Before doing so it is important to highlight one issue: this method assumes that being\\r\\nabove the average is a positive condition for all the features and this is guaranteed by the\\r\\ndefinition of the features given in Section 4.2.\\r\\nNow let A indicate a user above the average, while B a user below the average. Then\\r\\nlet F be the set of features and zfu\\r\\nthe z-score for the feature f 2 F for user u 2 U. It is\\r\\n72\\r\\npossible to associate to each user u a label (or to a class) lu according to this formula:\\r\\nlu =\\r\\n(\\r\\nA if mean\\r\\nf2F\\r\\nfzfu\\r\\ng \\x15 0\\r\\nB otherwise\\r\\n8u 2 U\\r\\nBy slightly exaggerating things, the former group could be interpreted as or called\\r\\nthe group of engaged/knowledgeable students (depending on the dataset), while the latter\\r\\nthe one of disengaged/uninformed students (see clusters centroids in Figure 4.14a). This\\r\\nlabelling strategy gave a silhouette score of 0.15 for engagement and 0.16 for knowledge,\\r\\nwhich are pretty low values. Yet it has been preferred to the previous approach because\\r\\nits centroids are easier to interpret and their repartition is more balanced. In any case,\\r\\nSMOTE [8] has been eventually used to oversample the minority class and reach perfect\\r\\nbalance between the classes.\\r\\n(a)\\r\\n(b)\\r\\nFigure 4.14: Clusters centroids (a) and clusters balance (b) for knowledge and\\r\\nengagement datasets. SMOTE has been subsequently applied to balance the clusters.\\r\\nModel Selection & Results\\r\\nAt this point a bag of models was trained and validated in order to identify the best performing\\r\\none. The chosen algorithms are three decision-tree based algorithm (Random-\\r\\n73\\r\\nForest, GradientBoosting and XGBoost) given their solidity, plus Logistic Regression and\\r\\nSupport Vector Machines because they are quite indicated in cases of binary classification.\\r\\nFor validation, to take the maximum advantage from the little amount of data Leave-\\r\\nOne-Out cross-validation was used. Given that the labels are perfectly balanced after\\r\\nresampling, accuracy has been chosen to evaluate the models. Table 4.26 shows the results\\r\\nfor the cross validation procedure for the five algorithms with default parameters. Because\\r\\nof the very high results of Logistic Regression, no further hyper-parameters tuning was\\r\\nperformed.\\r\\nCV Accuracy (%)\\xb1 Variance (%)\\r\\nEngagement Knowledge\\r\\nRandom Forest 85.70\\xb1 12.24 89.39\\xb1 9.48\\r\\nGradient Boosting 71.42\\xb1 20.40 75.76\\xb1 18.37\\r\\nXGBoost 82.14\\xb1 14.67 89.39\\xb1 9.48\\r\\nLogistic Regression 96.43\\xb1 3.44 93.94\\xb1 5.69\\r\\nSVM 69.64\\xb1 21.14 69.70\\xb1 21.12\\r\\nTable 4.26: Cross-validation accuracies scores (with variance) for all algorithms tested.\\r\\nAfter having fitted the Logistic Regressor with all the available training data, the\\r\\nweights of the classifier have been used as a measure of feature importance. The weights\\r\\n(normalized over the sum) for the two models are shown in Figure 4.15. From these results\\r\\nit seems that the most important indicators for engagement are the number of messages\\r\\nand comments written, while for knowledge the number of likes received by the teacher is\\r\\nfound to be most significant by far. Interestingly enough, all the text mining features happen\\r\\nto be of very little importance, both for engagement and knowledge. Also, the quiz\\r\\ngrades seem to be a better predictor of knowledge than the assignment grades although\\r\\nintuitively this should not be true. Indeed, since quizzes have multiple-choice questions\\r\\nwhere students are automatically scored based on the number of correct answers, students\\r\\nmay even get high grades by just random guessing; on the other hand, assignments are in\\r\\nthe form of essays and are personally graded by the teacher who should be able to better\\r\\ndistinguish between a knowledgeable and a mediocre student.\\r\\nPlease notice that associating an importance to each variable is not only useful from a\\r\\ndidactic or academic point of view but also from a practical one, because it makes possible\\r\\nto compute weighted sums of students\\x92 features and bring engagement and knowledge\\r\\ndown to single numbers. Let FE and FK denote the two sets of engagement and knowledge\\r\\nfeatures, respectively. Now let wf be the weight of a given feature f, belonging to\\r\\neither one of the two groups of features, and let zfu\\r\\ndenote the value of feature f for a\\r\\ngiven user u. It is then possible to score the knowledge (ku) or the engagement (eu) of a\\r\\nstudent by taking the weighted sum of his features, like so:\\r\\nku =\\r\\nX\\r\\nf2FK\\r\\nwf zfu\\r\\nand eu =\\r\\nX\\r\\nf2FE\\r\\nwf zfu\\r\\n74\\r\\nFigure 4.15: Final features relative importance for both engagement and knowledge\\r\\nfeatures.\\r\\nIn turn, condensing knowledge and engagement in single numbers is extremely powerful\\r\\nbecause it allows to plot each student in a 2D space knowledge-engagement and allows\\r\\nteachers and academic staff to have a rough idea of students\\x92 learning journey with just\\r\\none glance. This indeed represents one of the business goals set in Section 1.2.1. Additionally,\\r\\nthe same plot can help answer the question whether knowledge and engagement\\r\\nare correlated in HIOPs. Figure 4.16 shows scatter plots for Z-normalized engagement\\r\\nand knowledge scores for all the students, computed over different intervals of time. It\\r\\nis possible to see that correlation grows almost linearly to the number of days and it is\\r\\nalways positive, thus confirming the general belief that more engaged students also learn\\r\\nand know more. It is interesting to see how data points in the first week seem to be more\\r\\nspread in a horizontal line, suggesting that at the beginning of the course knowledge is\\r\\nmore or less leveled while there are more differences in terms of engagement, with some\\r\\nstudents maybe waiting for some time before getting involved. As the time passes by, also\\r\\nthe slope of the regression plots (recall that in case of standardized data the slope of the\\r\\nlinear regressor is equal to the correlation) grows, indicating that by the end of the course\\r\\nmore engaged students also learned more.\\r\\n4.3.3 Students\\x92 Performance Prediction\\r\\nThe second part of this subsection dedicated to modelling deals with the implementation\\r\\nof a predictive model for student success. As already pointed out in previous sections,\\r\\nsuccess is a very vast concept that can be declined and interpreted in many different ways.\\r\\nFor the purpose of this work, success is equivalent to final grades. That is, the goal of\\r\\npredictive modelling here is to predict students\\x92 final grades from a bunch of features\\r\\nalready outlined in Section 4.2.1. Because final grades are available for all the students,\\r\\nwe are discussing about supervised learning. Also, since grades have been discretized in\\r\\n75\\r\\n(a) (b)\\r\\n(c) (d)\\r\\n(e)\\r\\nFigure 4.16: Engagement and knowledge scatter plot over number of days. It is possible\\r\\nto notice that correlation grows with the passing of the weeks.\\r\\n76\\r\\n4 classes, the problem could be described as multi-class classification.\\r\\nThe dataset used for this problem has been already described in Section 4.2.2. After\\r\\nbeing preprocessed in Section 4.2, the only pending issue is the problem of unbalanced\\r\\nclasses. Eventually SMOTE has been chosen as default method for class balancing.\\r\\nModel Selection & Results\\r\\nModel selection has been done focusing on three main machine learning algorithms, all\\r\\ndecision-tree based: RandomForest, GradientBoosting and XGBoost. This is because\\r\\naccording to literature [42], decision tree-based algorithms are robust to noise which in\\r\\nthis case may have been introduced by the synthetic data sampling discussed earlier, and\\r\\nbecause ensemble methods are generally considered as the best performing models since\\r\\nthey put together the results of multiple classifiers.\\r\\nTo explore more in depth the potential of each algorithm, hyper-parameters have been\\r\\ntuned using random search: for each parameter a set of potential values (or ranges) has\\r\\nbeen defined, then 100 experiments have been run per each algorithm, and in each run a\\r\\nrandom set of hyper-parameters have been created and fed into the classifier for training.\\r\\nTo rank all the models and select the best one, two metrics have been taken into account:\\r\\nthe first is accuracy, that is the percentage of students correctly classified over the total\\r\\nnumber of predictions; the second is what it has been called safe predictions ratio, that\\r\\nis the percentage of students predicted with a lower score than the true one over the total\\r\\nnumber of wrong predictions. Since the latter is considered to be slightly more important\\r\\nthan the second, the ultimate metric \\x16 used is an F2 score of the two:\\r\\n\\x16 = 5 \\x01\\r\\n\\x0b \\x01 \\x1b\\r\\n4\\x0b \\x01 \\x1b\\r\\n2 [0; 1]\\r\\nFor validation, 10-(Stratified)Fold Cross-Validation has been used. This is because in\\r\\na multi-class scenario, having few examples in the validation (test) set might have a really\\r\\nhigh impact on the evaluation metrics. 10-Fold Cross-Validation ensures that 90% of data\\r\\nis always used for training and that at least 8 class-balanced samples are left for testing\\r\\n(10% of 88, the whole training test size).\\r\\nFigure 4.17 shows the value of the F2 score for the best ranked models for each algorithm\\r\\nand each interval of time. The best models have all mean above 0.6, with the best\\r\\nscenario being the Gradient Boosting at 21 days scoring 0.8; these are all good values,\\r\\nsuggesting low bias. However, the variance (represented by color bands in the line plot\\r\\nand by black error bars in the bar plot) is extremely high, meaning that different runs of\\r\\ncross-validation gave very distant result: this is a clear indicator of over-fitting. As it is\\r\\npossible to see in Figure 4.17a, the variance is so high that in most of the cases scores\\r\\nbelow 0.5 are possible, which makes the model unusable. It must be then acknowledged\\r\\nthat with the data currently available it was not possible to build a model valid enough to\\r\\nfulfil entirely the goals set at the beginning. Given the acceptable low bias but the serious\\r\\nover-fitting, it may be assumed that the main problem of the model is that not enough\\r\\ntraining data were available, which makes perfectly sense.\\r\\nRanking instead the models by plain accuracy, it is possible to get another set of \"best\\r\\nmodels\". In this case models are tuned to get the highest number of perfect matches, not\\r\\n77\\r\\n(a) (b)\\r\\nFigure 4.17: F2 scores trend (a) and bar (b) plot for multiple algorithms and time\\r\\nintervals.\\r\\ncaring about the type of the wrong predictions. Although it is not the best type of model\\r\\nfor early alert, they still can give useful information. After parameter tuning, the results\\r\\nof the best ranked models are showed in Figure 4.18. Once again it seems that the best\\r\\nresults are given by the Gradient Boosting classifier at 21 days (actually, ex aequo with\\r\\nRandom Forest Classifier at 28 days). In general all models seem to perform quite well in\\r\\nterms of accuracy, with values constantly above 60% (considering variance). Also, in this\\r\\ncase there is a more emphasized trend suggesting that the quality of predictions slightly\\r\\nimproves with time, as student complete the course.\\r\\nTo answer to BG.3. from Section 1.2.1, extracting features importances from these\\r\\nmodels allows to get an idea of the best predictors of student success at various points\\r\\nin time. Figure 4.19 shows the relative importance of each feature in the most accurate\\r\\nmodels trained (Gradient Boosting classifier with 21-days data and Random Forest classifier\\r\\nwith 28-days data - see Table 4.27). Not surprisingly, the average assignment grade\\r\\nand the average quiz grades are among the best predictor in both models (indeed, grades\\r\\nrepresent the way final labels are determined, although in another format than plain average);\\r\\nmore interestingly, however, this same piece of information could indicate that\\r\\nthe average grades at the end of the second week are already a good reflection of final\\r\\nperformance.\\r\\n4.4 Deployment\\r\\nDeployment is the phase where all the work done in the previous sections is finally put\\r\\ninto practical use and thanks to which the clients can possibly fulfil the business goals that\\r\\nwere set at the beginning.\\r\\nFirst of all, as already described in Section 4.1.1 all the knowledge gained during\\r\\nthe data collection and pre-processing phases described in the previous section led to the\\r\\n78\\r\\n(a) (b)\\r\\nFigure 4.18: Accuracies trend (a) and bar (b) plot for multiple algorithms and time\\r\\nintervals.\\r\\n(a) (b)\\r\\nFigure 4.19: Features importances for the two best model (in terms of accuracy) selected:\\r\\nGradient Boosting Classifier at 21 days (a) and Random Forest Classifier at 28 days (b).\\r\\n79\\r\\nCV Accuracy(%)\\xb1 Variance(%)\\r\\ndays 7 14 21 28\\r\\nGradient Boosting 75.00\\xb1 17.13 73.86\\xb1 9.42 85.23\\xb1 15.68 79.55\\xb1 15.64\\r\\nRandom Forest 79.55\\xb1 19.30 75.00\\xb1 11.72 79.55\\xb1 12.43 85.23\\xb1 8.68\\r\\nXGBoost 72.73\\xb1 14.09 75.00\\xb1 11.72 80.68\\xb1 14.95 81.82\\xb1 13.79\\r\\nTable 4.27: Summary table of cross-validation accuracy scores (with variance) for the\\r\\nbest models per each type of algorithm and time interval considered.\\r\\ncreation a software library in Python specifically designed to automatize these steps for\\r\\nany course (intake) available at any given point in time and to store all the information\\r\\nof proprietary servers. The potential of this library is exploited through a script that runs\\r\\nautomatically every day and given a predefined list of active courses updates daily all\\r\\ntheir information. It must be recalled that before the implementation of this solution, data\\r\\ncollection was extremely cumbersome, taking several hours each week to have only partial,\\r\\nnon-refined, coarse-grained information. Therefore, this piece of software represents\\r\\nalready a huge step forward within the institution.\\r\\nYet, to make all these information available to professors and project managers, the\\r\\nchosen solution was to design an user-friendly analytics dashboard. The dashboard is\\r\\naccessible through a dedicated web page and it contains a set of predefined plots presenting\\r\\nseveral key factors, figures and information about the courses, some examples of\\r\\nwhich have already been shown in Figure 4.6, Figure 4.5 and Figure 4.16. Since the same\\r\\nscript that updates the various datasets generates also all the plots just mentioned, also the\\r\\ndashboard is updated daily so that the staff can have access to the very latest information\\r\\nwhenever they log into the dashboard.\\r\\nThe only thing that was not automatized yet is students\\x92 grades prediction. Given the\\r\\nmediocre results presented in the previous section it has rather been left as an open line\\r\\nof work for the future.\\r\\n80\\r\\n5 CONCLUSIONS & FUTURE WORK\\r\\n5.1 Conclusions\\r\\nIn this thesis, I presented the full development of a data mining project I carried out for\\r\\none of the major institutions in the field of private Higher Education in Spain, IE Business\\r\\nSchool, with the objective of providing them with a data-driven decision support system.\\r\\nThe major problem addressed by this work is how to monitor students\\x92 performance and\\r\\nempower academic staff and teachers to take preventive and corrective actions using a\\r\\ndata-driven approach. Also, it set an important milestone for the adoption of learning\\r\\nanalytics techniques within the institution.\\r\\nThe first element of novelty and most characteristic trait of this work is certainly\\r\\nits application context. Indeed, most of previous work in literature either focused on\\r\\nblended courses or on Massive Online Open Courses (MOOCs), while this thesis paid\\r\\nspecific attention to the case of Small Private Online Courses (SPOCs) which are specially\\r\\ndesigned to offer tailor-made education and ad-hoc assistance to a small group of students,\\r\\nsuch as in the case of IEXL HIOPs.\\r\\nThe other element of innovation and uniqueness of this thesis is the proposed conceptualization\\r\\nand modeling of students\\x92 learning journey, and more specifically of students\\x92\\r\\nknowledge, which has been differentiated from the concept of student performance and\\r\\nelevated to something more complex. According to this last theorization, student knowledge\\r\\ntakes into account not only assignment grades (i.e. performance) but also skills (such\\r\\nas expressing writing skills), actions (number of contents viewed) and behavioural factors\\r\\n(average time spent on video lectures before changing content), among other things.\\r\\nThis brings us to the third key novelty of this work. Both knowledge and engagement\\r\\nhave been modelled as two distinct sets of variables, each with its own weight and importance\\r\\nwithin the corresponding concept. Rather than assigning (i.e. dictating) arbitrary\\r\\nweights to these concepts, this thesis presented a strategy to derive knowledge and engagement\\r\\nfeatures importances directly from data, thus offering a data-driven approach to\\r\\nlearning journey modeling.\\r\\nThanks to this approach it has been possible to investigate whether students\\x92 engagement\\r\\nand knowledge are indeed correlated as often assumed. As a matter of fact, if the\\r\\nweight are dictated they may be calibrated to match one\\x92s own assumption, while if they\\r\\nare derived from data the results are necessarily more objective. Following the latter approach,\\r\\nit was concluded that knowledge and engagement are indeed correlated.\\r\\nAlthough innovative, it is important to acknowledge its limitations right from the start.\\r\\n81\\r\\nFirst of all, being knowledge and engagement ensembles of different variables arbitrarily\\r\\nassociated to one or other concept, this means that some features may have been assigned\\r\\nto the less appropriate one. Secondly, since there is no limit to the number of variables\\r\\nthat is possible to consider, this leaves room for an infinite number of possibilities as it is\\r\\nalways possible to add or remove features. This led us to conclude that there is no right\\r\\nor wrong definition of knowledge nor engagement, but rather good or bad ones. In this\\r\\nsense, the definition here proposed is quite mediocre, as confirmed by the relatively low\\r\\nquality of the clusters and by some counter-intuitive results showed in Section 4.3.2. In\\r\\norder to improve such definitions we believe that adding new variables is fundamental,\\r\\nyet to do this it is fundamental to have more data points (to avoid data sparseness and\\r\\ncurse of dimensionality discussed in Chapter 3).\\r\\nFinally, another important contribution relies in the definition of a predictive model\\r\\nfor student success and the design of a custom a performance metric for early-warning\\r\\nsystems. As a matter of fact, while previous works opted either for a regression model\\r\\nfor final students\\x92 grades or for a binary classification model for failing students, in this\\r\\ncase four multi-class classification problems were modelled, one per each week of course.\\r\\nAlso, while accuracy is generally regarded as the most accepted measure of performance,\\r\\nit is important to notice that in case of education and early-warning systems, real success\\r\\nlays in not over-estimating student success rather than guessing exact grades. Here-hence,\\r\\nthe need for the introduction of a new performance metric that could account for both\\r\\naccuracy and \"safe\" predictions. It has been shown that although all the models had on\\r\\naverage very good results, their variance was so high that none of them actually matched\\r\\nthe success criteria set at the beginning. Given the low bias and the high variance, the\\r\\nreason of such failure have to be ascribed to over-fitting, which in turn is probably the\\r\\nresult (once again) of the very small amount of training data available. Yet, fine-tuning\\r\\nthe models for plain accuracy gave much better results which may be still of some use for\\r\\nteachers and staff, for example extracting the best predictors of student performance as it\\r\\nhas been shown in Section 4.3.3.\\r\\nFinally, on a more practical and technical side, the major contribution offered to the\\r\\ninstitution with the work of these months is a software tool that allows for automatized\\r\\ndata collection and pre-processing, as well as automated plots generation, which can be\\r\\nused for any other future intake or course at IEXL. This will allow IEXL to save personnel\\r\\nand resources even in the case the number of students will grow.\\r\\n5.2 Future Work\\r\\nThe work here discussed openly represents the starting point of a very long road for the\\r\\nadoption of Learning Analytics as a tool to enhance the quality of the courses at IE and\\r\\nIEXL. Therefore, the future improvements are many.\\r\\nFirst of all, the data used for this course was coming from a very limited pool of six\\r\\nintakes of the same course that was chosen as a pilot. The intention is to extend the same\\r\\nprocess to all future intakes of the same courses, so to improve the quantity of the data and\\r\\npossibly the quality of the results. Apart from adding new intakes of the same courses,\\r\\nthe second natural development of this work is to create new models for other courses.\\r\\n82\\r\\nIndeed, a new course would imply the creation of a completely new model: that is because\\r\\nits structure, its audience, its use of discussion forums may be completely different and\\r\\nthe features may have to be changed. Also, a different teacher would imply a different\\r\\ngrading criteria and thus a slightly different type of \"labelling\" for a classifier.\\r\\nRelated to this, as mentioned in Section 4.4 there is also the need to automatize models\\r\\npredictions, so to have a set of predictions by the end of each week using the best model\\r\\navailable for the given interval of time. This, however, will be possible only after having\\r\\nimproved the quality of results and reduced the serious over-fitting issues observed in\\r\\nSection 4.3.3, which in turn will be possible only once more data will be available.\\r\\nChanging topic and speaking about features, it emerged in Section 4.2 that some of the\\r\\nfeatures have been slightly adapted (see given rating) while some others have been either\\r\\ndiscarded (see read posts) or never computed because the limitation of the data available\\r\\n(for example, exact time spent on platform). Here-hence, the need to improve in the future\\r\\ndata collection and preparation so to make possible the refinement of existing features\\r\\nand the introduction of new others. As an example apart from those already mentioned,\\r\\nthe number and the length of students\\x92 sessions: in Section 4.1 it was found out that the\\r\\nauthentication events log registering all students\\x92 logins and logouts was actually unusable\\r\\nbecause students were not closing their sessions; for future intakes, the option of setting\\r\\na maximum duration for students sessions in Canvas should be taken into consideration\\r\\nand discussed. It is also important to recall that adding new features could improve the\\r\\nquality of the current definitions of knowledge and engagement.\\r\\nAbout the features here considered, they are clearly a minuscule subset of all the\\r\\nvariables that it is possible to extract from the vast amount of datasets available. This\\r\\nmodest number has to be ascribed also to the limited number of data points available; the\\r\\nintention is to keep trying to add potentially more informative and complex features as the\\r\\nnumber of students under analysis will grow, too. By means of example, another major\\r\\nimprovement for the feature currently in use is the algorithm used to score the intrinsic\\r\\nknowledge of each student message. The solution proposed in Section 4.2 is clearly too\\r\\nsimplistic and has many defects, as it is still possible to use words suggested by the teacher\\r\\nwithout conveying any thoughtful message. In this sense, some steps have already been\\r\\nmade: the plan is to use teachers\\x92 likes to messages as a measure of their validity so\\r\\nto train a classifier using teacher likes as labels and text-mined variables as features and\\r\\npossibly identify which features are the best predictors of \"message validity\".\\r\\n83'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
